{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T11:20:32.257032300Z",
     "start_time": "2026-01-11T10:52:54.617433800Z"
    },
    "id": "NIwT79NEu6qj"
   },
   "outputs": [],
   "source": [
    "# install the necessary libraries\n",
    "# !pip install gym\n",
    "# !pip install pygame\n",
    "# !pip install opencv-python\n",
    "# !pip install flappy-bird-gymnasium\n",
    "# !pip install matplotlib\n",
    "# !pip install torch torchvision\n",
    "# %pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "mNsP66_wvBbk"
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import random\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import flappy_bird_gymnasium\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "raAx8302vJ75",
    "outputId": "2bd78d85-a2ab-458c-af90-c88fde4ec766"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: cpu\n"
     ]
    }
   ],
   "source": [
    "# configure the device (GPU if its available if not, CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "eGoaiZhzvyhP"
   },
   "outputs": [],
   "source": [
    "# configure seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "gvKP690lvV7N"
   },
   "outputs": [],
   "source": [
    "# image preprocessing\n",
    "class ImagePreprocessor:\n",
    "  \"\"\"\n",
    "  class to preprocess the images from the flappy bird environment\n",
    "  converts images from rgb to grayscale, resizes and normalizes\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, width=84, height=84):\n",
    "    self.width = width\n",
    "    self.height = height\n",
    "\n",
    "  def preprocess(self, frame):\n",
    "    \"\"\"\n",
    "    preprocess a frame of the game\n",
    "    arguments:\n",
    "      frame: frame of the game (H, W, 3) or (H, W) if already grayscale\n",
    "    returns:\n",
    "      preprocessed frame (1, H, W) normalized\n",
    "    \"\"\"\n",
    "    # convert to grayscale (handle different input formats)\n",
    "    if len(frame.shape) == 2:\n",
    "      # already grayscale\n",
    "      gray = frame\n",
    "    elif len(frame.shape) == 3:\n",
    "      if frame.shape[2] == 1:\n",
    "        # already grayscale but with extra dimension\n",
    "        gray = frame[:, :, 0]\n",
    "      elif frame.shape[2] == 3:\n",
    "        # convert rgb to grayscale\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "      elif frame.shape[2] == 4:\n",
    "        # convert rgba to grayscale\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_RGBA2GRAY)\n",
    "      else:\n",
    "        raise ValueError(f\"unsupported number of channels: {frame.shape[2]}\")\n",
    "    else:\n",
    "      raise ValueError(f\"unsupported frame shape: {frame.shape}\")\n",
    "\n",
    "    # resize\n",
    "    resized = cv2.resize(gray, (self.width, self.height),\n",
    "                        interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # normalize (values between 0 and 1)\n",
    "    normalized = resized.astype(np.float32) / 255.0\n",
    "\n",
    "    # add a channel dimension (H, W) -> (1, H, W)\n",
    "    preprocessed = np.expand_dims(normalized, axis=0)\n",
    "\n",
    "    return preprocessed\n",
    "\n",
    "  def preprocess_batch(self, frames):\n",
    "    \"\"\"preprocesses a batch of frames\"\"\"\n",
    "    return np.array([self.preprocess(frame) for frame in frames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xwHf1RVzw4zi",
    "outputId": "1a5cf11c-ca9e-47fa-f2a8-456ca1bfefdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN model created\n",
      "Number of parameters: 1,685,154\n"
     ]
    }
   ],
   "source": [
    "# convolutional neural network\n",
    "class DQN(nn.Module):\n",
    "  \"\"\"\n",
    "  Deep Q-Network with a convolutional architecture to process imaghes.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, input_channels=4, n_actions=2):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      input_channels: number of stacked frames (to capture movement)\n",
    "      n_actions: number of possible actiones (0=do nothing, 1=jump)\n",
    "    \"\"\"\n",
    "    super(DQN, self).__init__()\n",
    "\n",
    "    # convolutional layers\n",
    "    self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=8, stride=4)\n",
    "    self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "    self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "\n",
    "    # compute the szie of the output of the convolutional layers\n",
    "    # for images 84x84\n",
    "    # after conv1:(84-8)/4 + 1 = 20\n",
    "    # after conv2:(20-4)/2 + 1 = 9\n",
    "    # after conv3:(9-3)/1 + 1 = 7\n",
    "    # total: 64 * 7 * 7 = 3136\n",
    "\n",
    "    conv_output_size = 64 * 7 * 7\n",
    "\n",
    "    # fully connected layers\n",
    "    self.fc1 = nn.Linear(conv_output_size, 512)\n",
    "    self.fc2 = nn.Linear(512, n_actions)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      x: input tensor (batch_size, channels, height, width)\n",
    "    returns:\n",
    "      Q-values for each action (batch_size, n_actions)\n",
    "    \"\"\"\n",
    "    # convolutional layers with relu\n",
    "    x = F.relu(self.conv1(x))\n",
    "    x = F.relu(self.conv2(x))\n",
    "    x = F.relu(self.conv3(x))\n",
    "\n",
    "    # flatten for fully connected layers\n",
    "    x = x.view(x.size(0), -1)\n",
    "\n",
    "    # fully connected layers\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = self.fc2(x)  # Sin activación en la salida (Q-values pueden ser cualquier número)\n",
    "\n",
    "    return x\n",
    "\n",
    "# create a test instance\n",
    "test_model = DQN(input_channels=4, n_actions=2).to(device)\n",
    "print(f\"DQN model created\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in test_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "wMC2w6WbyNWE"
   },
   "outputs": [],
   "source": [
    "# replay buffer\n",
    "class ReplayBuffer:\n",
    "  \"\"\"\n",
    "  Memory buffer to store experience tuples (transitions) and sample mini-batches\n",
    "  Implements the concept of experience replay from deepmind\n",
    "  \"\"\"\n",
    "  def __init__(self, capacity=100000):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      capacity: maximum buffer size\n",
    "    \"\"\"\n",
    "    self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "  def push(self, state, action, reward, next_state, done):\n",
    "    \"\"\"\n",
    "    adds a transition to the buffer\n",
    "    arguments:\n",
    "        state: current state (array)\n",
    "        action: action taken (int)\n",
    "        reward: recieved rewars (float)\n",
    "        next_state: next state (numpy array)\n",
    "        done: if the episode finished (bool)\n",
    "    \"\"\"\n",
    "    self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "  def sample(self, batch_size):\n",
    "    \"\"\"\n",
    "    samples a random minibatch from the buffer\n",
    "    arguments:\n",
    "      batch_size: batch size\n",
    "    returns:\n",
    "      tensors tuples (states, actions, rewards, next_states, dones)\n",
    "    \"\"\"\n",
    "    # random sampling\n",
    "    batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "    # separate the components\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "    # convert to tensors\n",
    "    states = torch.FloatTensor(np.array(states)).to(device)\n",
    "    actions = torch.LongTensor(actions).to(device)\n",
    "    rewards = torch.FloatTensor(rewards).to(device)\n",
    "    next_states = torch.FloatTensor(np.array(next_states)).to(device)\n",
    "    dones = torch.FloatTensor(dones).to(device)\n",
    "\n",
    "    return states, actions, rewards, next_states, dones\n",
    "\n",
    "  def __len__(self):\n",
    "    \"\"\"returns the current size of the buffer\"\"\"\n",
    "    return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "9N5rFPwh0Fox"
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "  \"\"\"\n",
    "  agent that implements the q-learning algorithm using a deep neural network\n",
    "  \"\"\"\n",
    "  def __init__(self, state_shape=(4, 84, 84), n_actions=2, learning_rate=0.00025, gamma=0.99, epsilon_start=1.0,\n",
    "               epsilon_end=0.01, epsilon_decay=0.995,buffer_capacity=100000, batch_size=32, target_update_freq=1000):\n",
    "\n",
    "    self.n_actions = n_actions\n",
    "    self.gamma = gamma\n",
    "    self.epsilon = epsilon_start\n",
    "    self.epsilon_end = epsilon_end\n",
    "    self.epsilon_decay = epsilon_decay\n",
    "    self.batch_size = batch_size\n",
    "    self.target_update_freq = target_update_freq\n",
    "    self.steps = 0\n",
    "\n",
    "    # neural networks (policy net and target net)\n",
    "    self.policy_net = DQN(state_shape[0], n_actions).to(device)\n",
    "    self.target_net = DQN(state_shape[0], n_actions).to(device)\n",
    "\n",
    "    # copy the initial weights to the target network\n",
    "    self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "    self.target_net.eval()  # target netwotk in evaluation mode\n",
    "\n",
    "    # adam optimizer and huber loss\n",
    "    self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)\n",
    "    self.criterion = nn.SmoothL1Loss()\n",
    "\n",
    "    # replay buffer\n",
    "    self.memory = ReplayBuffer(buffer_capacity)\n",
    "\n",
    "    # metrics\n",
    "    self.losses = []\n",
    "    self.rewards_history = []\n",
    "\n",
    "  def select_action(self, state, training=True):\n",
    "    \"\"\"\n",
    "    selects an action using the epsilon greedy strategy\n",
    "    arguments:\n",
    "      state: current state (numpy array)\n",
    "      training: if its in training mode (uses epsilon-greedy)\n",
    "    returns:\n",
    "      selectd action (int)\n",
    "    \"\"\"\n",
    "    if training and random.random() < self.epsilon:\n",
    "      # exploration: choose a random action\n",
    "      return random.randrange(self.n_actions)\n",
    "    else:\n",
    "      # explotation: choose the action with the best Q-value\n",
    "      with torch.no_grad():\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        q_values = self.policy_net(state_tensor)\n",
    "        return q_values.argmax(1).item()\n",
    "\n",
    "  def store_transition(self, state, action, reward, next_state, done):\n",
    "    \"\"\"stores a transition in the replay buffer\"\"\"\n",
    "    self.memory.push(state, action, reward, next_state, done)\n",
    "\n",
    "  def update(self):\n",
    "    \"\"\"\n",
    "    updates the neural network using Q-learning\n",
    "    implements the Bellman equation: Q(s,a) = r + γ * max(Q(s',a'))))\n",
    "    \"\"\"\n",
    "    # verify if there are enough experiences\n",
    "    if len(self.memory) < self.batch_size:\n",
    "      return None\n",
    "\n",
    "    # sample a mini-batch\n",
    "    states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "\n",
    "    # calculate current Q-values Q(s,a)\n",
    "    current_q_values = self.policy_net(states).gather(1, actions.unsqueeze(1))\n",
    "\n",
    "    # calculate goal Q-values r + γ * max(Q(s',a'))\n",
    "    with torch.no_grad():\n",
    "      next_q_values = self.target_net(next_states).max(1)[0]\n",
    "      # if the episode finished (done=1), future value is 0\n",
    "      target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "    # calculate the loss (current Q - goal Q)\n",
    "    loss = self.criterion(current_q_values.squeeze(), target_q_values)\n",
    "\n",
    "    # optimization\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # gradient clipping for stability\n",
    "    torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 10)\n",
    "\n",
    "    self.optimizer.step()\n",
    "\n",
    "    # save the losses\n",
    "    self.losses.append(loss.item())\n",
    "\n",
    "    # update the target network periodically\n",
    "    self.steps += 1\n",
    "    if self.steps % self.target_update_freq == 0:\n",
    "      self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "  def decay_epsilon(self):\n",
    "    \"\"\"reduces epsiolon to explore less with time\"\"\"\n",
    "    self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "  def save(self, filepath):\n",
    "    \"\"\"saves the trained model\"\"\"\n",
    "    torch.save({\n",
    "      'policy_net_state_dict': self.policy_net.state_dict(),\n",
    "      'target_net_state_dict': self.target_net.state_dict(),\n",
    "      'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "      'epsilon': self.epsilon,\n",
    "      'steps': self.steps\n",
    "    }, filepath)\n",
    "    print(f\"Model saved in {filepath}\")\n",
    "\n",
    "  def load(self, filepath):\n",
    "    \"\"\"loads a trained model\"\"\"\n",
    "    checkpoint = torch.load(filepath, map_location=device)\n",
    "    self.policy_net.load_state_dict(checkpoint['policy_net_state_dict'])\n",
    "    self.target_net.load_state_dict(checkpoint['target_net_state_dict'])\n",
    "    self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    self.epsilon = checkpoint['epsilon']\n",
    "    self.steps = checkpoint['steps']\n",
    "    print(f\"Model loaded from {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "z1jz7C5WULn-"
   },
   "outputs": [],
   "source": [
    "  # environment wrapper\n",
    "class FlappyBirdWrapper:\n",
    "    \"\"\"\n",
    "    wrapper for the flappy bird environment that includes:\n",
    "    - image preprocessing\n",
    "    - frame stacking (stack frames to capture movement)\n",
    "    - personalized rewards\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_frames=4):\n",
    "      \"\"\"\n",
    "      arguments:\n",
    "        n_frames: number of consective frames to stack\n",
    "      \"\"\"\n",
    "      # create the environment with render mode to get pixels\n",
    "      self.env = gym.make(\"FlappyBird-v0\", render_mode=\"rgb_array\")\n",
    "      self.n_frames = n_frames\n",
    "      self.preprocessor = ImagePreprocessor(84, 84)\n",
    "\n",
    "      # frame buffer\n",
    "      self.frame_buffer = deque(maxlen=n_frames)\n",
    "\n",
    "      # step counter\n",
    "      self.total_steps = 0\n",
    "\n",
    "    def reset(self):\n",
    "      \"\"\"\n",
    "      resets the environment\n",
    "      returns:\n",
    "        initial state (stacked frames)\n",
    "      \"\"\"\n",
    "      obs, _ = self.env.reset()\n",
    "\n",
    "      # get the visual frame (render) instead of the vectorial observation\n",
    "      visual_frame = self.env.render()\n",
    "\n",
    "      # preprocess the initial frame\n",
    "      processed_frame = self.preprocessor.preprocess(visual_frame)\n",
    "\n",
    "      # fill the buffer with the same initial frame\n",
    "      for _ in range(self.n_frames):\n",
    "        self.frame_buffer.append(processed_frame)\n",
    "      self.total_steps = 0\n",
    "\n",
    "      return self._get_state()\n",
    "\n",
    "    def step(self, action):\n",
    "      \"\"\"\n",
    "      exectutes an action in the environment\n",
    "\n",
    "      arguments:\n",
    "        action: action to execute (0 o 1)\n",
    "      returns:\n",
    "        tuple (state, reward, done, info)\n",
    "      \"\"\"\n",
    "      obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "      done = terminated or truncated\n",
    "\n",
    "      # get the visual frame (render) instead of the vectorial observation\n",
    "      visual_frame = self.env.render()\n",
    "\n",
    "      # preprocess the frame\n",
    "      processed_frame = self.preprocessor.preprocess(visual_frame)\n",
    "\n",
    "      # updatae the buffer\n",
    "      self.frame_buffer.append(processed_frame)\n",
    "\n",
    "      # personalized reward\n",
    "      # penalize diying and reward surviving\n",
    "      if done:\n",
    "          custom_reward = -10  # penalization for dying\n",
    "      else:\n",
    "          custom_reward = 0.1  # small reward for surviving\n",
    "\n",
    "      # use the games original reward (go though the tube) + the personalized reward\n",
    "      total_reward = reward + custom_reward\n",
    "\n",
    "      self.total_steps += 1\n",
    "\n",
    "      return self._get_state(), total_reward, done, info\n",
    "\n",
    "    def _get_state(self):\n",
    "        \"\"\"\n",
    "        obtains the current state (stacked frmaes).\n",
    "        returns:\n",
    "          array of stacked frames (n_frames, height, width)\n",
    "        \"\"\"\n",
    "        return np.concatenate(list(self.frame_buffer), axis=0)\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"renders the environment\"\"\"\n",
    "        return self.env.render()\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"closes the environment\"\"\"\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "QCe5nhUgWj-A"
   },
   "outputs": [],
   "source": [
    "# training function\n",
    "def train_dqn(agent, env, n_episodes=1000, max_steps=10000, save_freq=100, print_freq=10):\n",
    "  \"\"\"\n",
    "  trains the DQN agent in the flappy bird environment\n",
    "  arguments:\n",
    "    agent: DQN agent\n",
    "    env: environment wrapper\n",
    "    n_episodes: number of training episodes\n",
    "    max_steps: max number of steps per episode\n",
    "    save_freq: saving frequence of the model\n",
    "    print_freq: stats printing sequence\n",
    "  returns:\n",
    "    list of the rewards for each episode\n",
    "  \"\"\"\n",
    "  episode_rewards = []\n",
    "  episode_lengths = []\n",
    "  best_reward = -float('inf')\n",
    "\n",
    "  for episode in range(1, n_episodes + 1):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_loss = []\n",
    "\n",
    "    for step in range(max_steps):\n",
    "      # Seleccionar acción\n",
    "      action = agent.select_action(state, training=True)\n",
    "\n",
    "      # Ejecutar acción\n",
    "      next_state, reward, done, info = env.step(action)\n",
    "\n",
    "      # Almacenar transición\n",
    "      agent.store_transition(state, action, reward, next_state, done)\n",
    "\n",
    "      # Actualizar red neuronal\n",
    "      loss = agent.update()\n",
    "      if loss is not None:\n",
    "        episode_loss.append(loss)\n",
    "\n",
    "      episode_reward += reward\n",
    "      state = next_state\n",
    "\n",
    "      if done:\n",
    "        break\n",
    "\n",
    "    # decay epsilon\n",
    "    agent.decay_epsilon()\n",
    "\n",
    "    # save the metrics\n",
    "    episode_rewards.append(episode_reward)\n",
    "    episode_lengths.append(step + 1)\n",
    "\n",
    "    # print the progress\n",
    "    if episode % print_freq == 0:\n",
    "      avg_reward = np.mean(episode_rewards[-print_freq:])\n",
    "      avg_length = np.mean(episode_lengths[-print_freq:])\n",
    "      avg_loss = np.mean(episode_loss) if episode_loss else 0\n",
    "\n",
    "      print(f\"Episode {episode}/{n_episodes} | \"\n",
    "            f\"Reward: {episode_reward:.2f} | \"\n",
    "            f\"Average ({print_freq} ep): {avg_reward:.2f} | \"\n",
    "            f\"Length: {step+1} | \"\n",
    "            f\"Epsilon: {agent.epsilon:.3f} | \"\n",
    "            f\"Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # save the best model\n",
    "    if episode_reward > best_reward:\n",
    "      best_reward = episode_reward\n",
    "      agent.save('best_flappy_model.pth')\n",
    "\n",
    "    # save the model periodically\n",
    "    if episode % save_freq == 0:\n",
    "      agent.save(f'flappy_model_ep{episode}.pth')\n",
    "\n",
    "  return episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "p2Z7KbVnWoVI"
   },
   "outputs": [],
   "source": [
    "# evaluation function\n",
    "def evaluate_agent(agent, env, n_episodes=10, render=False):\n",
    "  \"\"\"\n",
    "  evaluates the performance of the agent trained without exploration\n",
    "  arguments:\n",
    "    agent: trained DQN agent\n",
    "    env: environment wrapper\n",
    "    n_episodes: number of evaluation episodes\n",
    "    render: if the game renders\n",
    "  returns:\n",
    "    list of the rewards and scores per episode\n",
    "  \"\"\"\n",
    "  episode_rewards = []\n",
    "  episode_scores = []\n",
    "\n",
    "  for episode in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    score = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "      # select the best action (without exploration)\n",
    "      action = agent.select_action(state, training=False)\n",
    "\n",
    "      # execute action\n",
    "      state, reward, done, info = env.step(action)\n",
    "      episode_reward += reward\n",
    "\n",
    "      # the score is usually the number of tubes gone through\n",
    "      if 'score' in info:\n",
    "        score = info['score']\n",
    "\n",
    "      if render:\n",
    "        frame = env.render()\n",
    "        plt.imshow(frame)\n",
    "        plt.axis('off')\n",
    "        plt.pause(0.01)\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "      episode_rewards.append(episode_reward)\n",
    "      episode_scores.append(score)\n",
    "\n",
    "      print(f\"Episode {episode+1}/{n_episodes} | \"\n",
    "            f\"Reward: {episode_reward:.2f} | \"\n",
    "            f\"Score: {score}\")\n",
    "\n",
    "  print(f\"\\nEvaluation results:\")\n",
    "  print(f\"Average reward: {np.mean(episode_rewards):.2f} ± {np.std(episode_rewards):.2f}\")\n",
    "  print(f\"Average score: {np.mean(episode_scores):.2f} ± {np.std(episode_scores):.2f}\")\n",
    "  print(f\"Best score: {np.max(episode_scores)}\")\n",
    "\n",
    "  return episode_rewards, episode_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Sa56QJyAWt23"
   },
   "outputs": [],
   "source": [
    "# graphing functions\n",
    "def plot_training_results(rewards, window=50):\n",
    "  \"\"\"\n",
    "  plots the results of the training\n",
    "  arguments:\n",
    "    rewards: list of rewards per episode\n",
    "    window: size of the moving average window\n",
    "  \"\"\"\n",
    "  plt.figure(figsize=(12, 5))\n",
    "\n",
    "  # rewards per epsiode\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plt.plot(rewards, alpha=0.3, label='Reward per episode')\n",
    "\n",
    "  # moving average\n",
    "  if len(rewards) >= window:\n",
    "      moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "      plt.plot(range(window-1, len(rewards)), moving_avg,\n",
    "              label=f'Moving average ({window} ep)', linewidth=2)\n",
    "\n",
    "  plt.xlabel('Episode')\n",
    "  plt.ylabel('Reward')\n",
    "  plt.title('Rewards during training')\n",
    "  plt.legend()\n",
    "  plt.grid(True, alpha=0.3)\n",
    "\n",
    "  # rewards histogram\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plt.hist(rewards, bins=30, edgecolor='black', alpha=0.7)\n",
    "  plt.xlabel('Reward')\n",
    "  plt.ylabel('Frequence')\n",
    "  plt.title('Rewards distribution')\n",
    "  plt.grid(True, alpha=0.3)\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.savefig('training_results.png', dpi=150, bbox_inches='tight')\n",
    "  plt.show()\n",
    "\n",
    "  print(\"Plot saved as 'training_results.png'\")\n",
    "\n",
    "def plot_evaluation_results(scores):\n",
    "  \"\"\"\n",
    "  plots the results of evaluation\n",
    "  arguments:\n",
    "    scores: list of the evaluation scores\n",
    "  \"\"\"\n",
    "  plt.figure(figsize=(10, 5))\n",
    "\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plt.bar(range(1, len(scores)+1), scores, color='skyblue', edgecolor='black')\n",
    "  plt.xlabel('Episode')\n",
    "  plt.ylabel('Score')\n",
    "  plt.title('Evaluation Scores')\n",
    "  plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plt.boxplot(scores)\n",
    "  plt.ylabel('Score')\n",
    "  plt.title('Scores distribution')\n",
    "  plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.savefig('evaluation_results.png', dpi=150, bbox_inches='tight')\n",
    "  plt.show()\n",
    "\n",
    "  print(\"Plot saved as 'evaluation_results.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GN53SZHuWzqK",
    "outputId": "f31abee0-adb3-46fc-c441-330bba9ecccf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Flappy Bird environment...\n",
      "Initializing DQN agent...\n",
      "\n",
      "Ready to train the DQN agent\n"
     ]
    }
   ],
   "source": [
    "# configuration an initialization\n",
    "\n",
    "# training hyperparameters\n",
    "HYPERPARAMETERS = {\n",
    "    'n_episodes': 1500,\n",
    "    'learning_rate': 0.00025,\n",
    "    'gamma': 0.99, # discount factor\n",
    "    'epsilon_start': 1.0,\n",
    "    'epsilon_end': 0.01,\n",
    "    'epsilon_decay': 0.99,\n",
    "    'buffer_capacity': 100000,\n",
    "    'batch_size': 64,\n",
    "    'target_update_freq': 500,\n",
    "    'n_frames': 4,\n",
    "    'max_steps': 10000,\n",
    "    'save_freq': 500,\n",
    "    'print_freq': 10\n",
    "}\n",
    "\n",
    "# create environment\n",
    "print(\"Creating Flappy Bird environment...\")\n",
    "env = FlappyBirdWrapper(n_frames=HYPERPARAMETERS['n_frames'])\n",
    "\n",
    "# create agent\n",
    "print(\"Initializing DQN agent...\")\n",
    "agent = DQNAgent(\n",
    "    state_shape=(HYPERPARAMETERS['n_frames'], 84, 84),\n",
    "    n_actions=2,\n",
    "    learning_rate=HYPERPARAMETERS['learning_rate'],\n",
    "    gamma=HYPERPARAMETERS['gamma'],\n",
    "    epsilon_start=HYPERPARAMETERS['epsilon_start'],\n",
    "    epsilon_end=HYPERPARAMETERS['epsilon_end'],\n",
    "    epsilon_decay=HYPERPARAMETERS['epsilon_decay'],\n",
    "    buffer_capacity=HYPERPARAMETERS['buffer_capacity'],\n",
    "    batch_size=HYPERPARAMETERS['batch_size'],\n",
    "    target_update_freq=HYPERPARAMETERS['target_update_freq']\n",
    ")\n",
    "\n",
    "print(\"\\nReady to train the DQN agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Gly4ebRPb5Rz",
    "outputId": "cf3fbe87-c6fc-4e79-b456-d7b1d337f290"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ionic\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "c:\\Users\\ionic\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in best_flappy_model.pth\n",
      "Model saved in best_flappy_model.pth\n",
      "Episode 10/1500 | Reward: -6.60 | Average (10 ep): -11.46 | Length: 50 | Epsilon: 0.904 | Loss: 0.0152\n",
      "Model saved in best_flappy_model.pth\n",
      "Episode 20/1500 | Reward: -12.00 | Average (10 ep): -11.82 | Length: 50 | Epsilon: 0.818 | Loss: 0.0053\n",
      "Episode 30/1500 | Reward: -12.60 | Average (10 ep): -12.60 | Length: 50 | Epsilon: 0.740 | Loss: 0.0052\n",
      "Episode 40/1500 | Reward: -13.80 | Average (10 ep): -12.66 | Length: 50 | Epsilon: 0.669 | Loss: 0.0055\n",
      "Episode 50/1500 | Reward: -13.20 | Average (10 ep): -12.30 | Length: 50 | Epsilon: 0.605 | Loss: 0.0046\n",
      "Episode 60/1500 | Reward: -13.20 | Average (10 ep): -12.00 | Length: 50 | Epsilon: 0.547 | Loss: 0.0049\n",
      "Episode 70/1500 | Reward: -12.00 | Average (10 ep): -12.42 | Length: 50 | Epsilon: 0.495 | Loss: 0.0052\n",
      "Model saved in best_flappy_model.pth\n",
      "Episode 80/1500 | Reward: -11.40 | Average (10 ep): -11.10 | Length: 50 | Epsilon: 0.448 | Loss: 0.0061\n",
      "Episode 90/1500 | Reward: -13.20 | Average (10 ep): -10.68 | Length: 50 | Epsilon: 0.405 | Loss: 0.0069\n",
      "Episode 100/1500 | Reward: -10.20 | Average (10 ep): -11.34 | Length: 50 | Epsilon: 0.366 | Loss: 0.0059\n",
      "Episode 110/1500 | Reward: -13.80 | Average (10 ep): -12.12 | Length: 50 | Epsilon: 0.331 | Loss: 0.0080\n",
      "Model saved in best_flappy_model.pth\n",
      "Episode 120/1500 | Reward: -12.60 | Average (10 ep): -11.16 | Length: 50 | Epsilon: 0.299 | Loss: 0.0051\n",
      "Episode 130/1500 | Reward: -6.00 | Average (10 ep): -10.92 | Length: 50 | Epsilon: 0.271 | Loss: 0.0052\n",
      "Episode 140/1500 | Reward: -10.80 | Average (10 ep): -11.46 | Length: 50 | Epsilon: 0.245 | Loss: 0.0054\n",
      "Episode 150/1500 | Reward: -6.00 | Average (10 ep): -10.18 | Length: 50 | Epsilon: 0.221 | Loss: 0.0130\n",
      "Episode 160/1500 | Reward: -12.60 | Average (10 ep): -10.92 | Length: 50 | Epsilon: 0.200 | Loss: 0.0124\n",
      "Episode 170/1500 | Reward: -7.20 | Average (10 ep): -8.70 | Length: 50 | Epsilon: 0.181 | Loss: 0.0118\n",
      "Episode 180/1500 | Reward: -10.80 | Average (10 ep): -9.66 | Length: 50 | Epsilon: 0.164 | Loss: 0.0085\n",
      "Model saved in best_flappy_model.pth\n",
      "Episode 190/1500 | Reward: -9.60 | Average (10 ep): -7.42 | Length: 50 | Epsilon: 0.148 | Loss: 0.0135\n",
      "Episode 200/1500 | Reward: -6.00 | Average (10 ep): -8.88 | Length: 50 | Epsilon: 0.134 | Loss: 0.0111\n",
      "Episode 210/1500 | Reward: -6.00 | Average (10 ep): -8.16 | Length: 50 | Epsilon: 0.121 | Loss: 0.0174\n",
      "Episode 220/1500 | Reward: -6.00 | Average (10 ep): -6.48 | Length: 50 | Epsilon: 0.110 | Loss: 0.0178\n",
      "Episode 230/1500 | Reward: -6.00 | Average (10 ep): -6.84 | Length: 50 | Epsilon: 0.099 | Loss: 0.0128\n",
      "Model saved in best_flappy_model.pth\n",
      "Episode 240/1500 | Reward: -4.20 | Average (10 ep): -5.48 | Length: 50 | Epsilon: 0.090 | Loss: 0.0137\n",
      "Episode 250/1500 | Reward: -6.00 | Average (10 ep): -6.06 | Length: 50 | Epsilon: 0.081 | Loss: 0.0156\n",
      "Episode 260/1500 | Reward: -4.80 | Average (10 ep): -6.36 | Length: 50 | Epsilon: 0.073 | Loss: 0.0157\n",
      "Model saved in best_flappy_model.pth\n",
      "Episode 270/1500 | Reward: -6.00 | Average (10 ep): -5.68 | Length: 50 | Epsilon: 0.066 | Loss: 0.0193\n",
      "Episode 280/1500 | Reward: -6.00 | Average (10 ep): -5.43 | Length: 50 | Epsilon: 0.060 | Loss: 0.0197\n",
      "Episode 290/1500 | Reward: -3.60 | Average (10 ep): -5.28 | Length: 50 | Epsilon: 0.054 | Loss: 0.0266\n",
      "Episode 300/1500 | Reward: -6.00 | Average (10 ep): -6.00 | Length: 50 | Epsilon: 0.049 | Loss: 0.0226\n",
      "Episode 310/1500 | Reward: -6.00 | Average (10 ep): -6.00 | Length: 50 | Epsilon: 0.044 | Loss: 0.0274\n",
      "Episode 320/1500 | Reward: -6.00 | Average (10 ep): -5.94 | Length: 50 | Epsilon: 0.040 | Loss: 0.0293\n",
      "Episode 330/1500 | Reward: -6.00 | Average (10 ep): -6.00 | Length: 50 | Epsilon: 0.036 | Loss: 0.0361\n",
      "Episode 340/1500 | Reward: -6.00 | Average (10 ep): -6.00 | Length: 50 | Epsilon: 0.033 | Loss: 0.0291\n",
      "Episode 350/1500 | Reward: -6.00 | Average (10 ep): -5.62 | Length: 50 | Epsilon: 0.030 | Loss: 0.0326\n",
      "Episode 360/1500 | Reward: -6.00 | Average (10 ep): -6.00 | Length: 50 | Epsilon: 0.027 | Loss: 0.0314\n",
      "Episode 370/1500 | Reward: -6.00 | Average (10 ep): -6.00 | Length: 50 | Epsilon: 0.024 | Loss: 0.0330\n",
      "Episode 380/1500 | Reward: -4.80 | Average (10 ep): -5.86 | Length: 50 | Epsilon: 0.022 | Loss: 0.0322\n",
      "Episode 390/1500 | Reward: -6.00 | Average (10 ep): -5.86 | Length: 50 | Epsilon: 0.020 | Loss: 0.0392\n",
      "Model saved in best_flappy_model.pth\n",
      "Episode 400/1500 | Reward: -6.00 | Average (10 ep): -5.62 | Length: 50 | Epsilon: 0.018 | Loss: 0.0714\n",
      "Episode 410/1500 | Reward: -2.40 | Average (10 ep): -4.76 | Length: 50 | Epsilon: 0.016 | Loss: 0.0952\n",
      "Model saved in best_flappy_model.pth\n",
      "Episode 420/1500 | Reward: -2.40 | Average (10 ep): -4.37 | Length: 50 | Epsilon: 0.015 | Loss: 0.1321\n",
      "Episode 430/1500 | Reward: -4.80 | Average (10 ep): -4.94 | Length: 56 | Epsilon: 0.013 | Loss: 0.0834\n",
      "Episode 440/1500 | Reward: -3.00 | Average (10 ep): -4.36 | Length: 50 | Epsilon: 0.012 | Loss: 0.0433\n",
      "Episode 450/1500 | Reward: -5.20 | Average (10 ep): -3.86 | Length: 54 | Epsilon: 0.011 | Loss: 0.0394\n",
      "Model saved in best_flappy_model.pth\n",
      "Model saved in best_flappy_model.pth\n",
      "Episode 460/1500 | Reward: -3.70 | Average (10 ep): -3.60 | Length: 93 | Epsilon: 0.010 | Loss: 0.0392\n",
      "Model saved in best_flappy_model.pth\n",
      "Model saved in best_flappy_model.pth\n",
      "Episode 470/1500 | Reward: -2.40 | Average (10 ep): -3.48 | Length: 50 | Epsilon: 0.010 | Loss: 0.0382\n",
      "Model saved in best_flappy_model.pth\n",
      "Episode 480/1500 | Reward: 1.50 | Average (10 ep): -2.46 | Length: 68 | Epsilon: 0.010 | Loss: 0.0468\n",
      "Episode 490/1500 | Reward: 1.50 | Average (10 ep): -3.61 | Length: 68 | Epsilon: 0.010 | Loss: 0.0605\n",
      "Episode 500/1500 | Reward: -6.00 | Average (10 ep): -4.23 | Length: 50 | Epsilon: 0.010 | Loss: 0.0709\n",
      "Model saved in flappy_model_ep500.pth\n",
      "Episode 510/1500 | Reward: -6.00 | Average (10 ep): -3.69 | Length: 56 | Epsilon: 0.010 | Loss: 0.0449\n",
      "Episode 520/1500 | Reward: -3.00 | Average (10 ep): -3.00 | Length: 50 | Epsilon: 0.010 | Loss: 0.0544\n",
      "Episode 530/1500 | Reward: -6.00 | Average (10 ep): -3.65 | Length: 50 | Epsilon: 0.010 | Loss: 0.0549\n",
      "Episode 540/1500 | Reward: -6.00 | Average (10 ep): -4.73 | Length: 50 | Epsilon: 0.010 | Loss: 0.0464\n",
      "Episode 550/1500 | Reward: -6.00 | Average (10 ep): -5.15 | Length: 50 | Epsilon: 0.010 | Loss: 0.0513\n",
      "Episode 560/1500 | Reward: -5.00 | Average (10 ep): -2.29 | Length: 52 | Epsilon: 0.010 | Loss: 0.0495\n",
      "Episode 570/1500 | Reward: -6.00 | Average (10 ep): -4.75 | Length: 50 | Epsilon: 0.010 | Loss: 0.0542\n",
      "Episode 580/1500 | Reward: -5.00 | Average (10 ep): -4.60 | Length: 52 | Epsilon: 0.010 | Loss: 0.0470\n",
      "Episode 590/1500 | Reward: -3.20 | Average (10 ep): -3.12 | Length: 52 | Epsilon: 0.010 | Loss: 0.0702\n",
      "Episode 600/1500 | Reward: -0.30 | Average (10 ep): -4.29 | Length: 65 | Epsilon: 0.010 | Loss: 0.0672\n",
      "Episode 610/1500 | Reward: -3.20 | Average (10 ep): -4.32 | Length: 52 | Epsilon: 0.010 | Loss: 0.0576\n",
      "Episode 620/1500 | Reward: -6.00 | Average (10 ep): -3.80 | Length: 50 | Epsilon: 0.010 | Loss: 0.0641\n",
      "Episode 630/1500 | Reward: -1.00 | Average (10 ep): -3.22 | Length: 60 | Epsilon: 0.010 | Loss: 0.0572\n",
      "Episode 640/1500 | Reward: -5.40 | Average (10 ep): -4.11 | Length: 56 | Epsilon: 0.010 | Loss: 0.0579\n",
      "Episode 650/1500 | Reward: -1.30 | Average (10 ep): -2.48 | Length: 69 | Epsilon: 0.010 | Loss: 0.0533\n",
      "Episode 660/1500 | Reward: -6.00 | Average (10 ep): -3.19 | Length: 50 | Epsilon: 0.010 | Loss: 0.0528\n",
      "Episode 670/1500 | Reward: -2.80 | Average (10 ep): -2.50 | Length: 51 | Epsilon: 0.010 | Loss: 0.0581\n",
      "Episode 680/1500 | Reward: -6.00 | Average (10 ep): -4.02 | Length: 50 | Epsilon: 0.010 | Loss: 0.0529\n",
      "Model saved in best_flappy_model.pth\n",
      "Episode 690/1500 | Reward: -5.40 | Average (10 ep): -2.23 | Length: 53 | Epsilon: 0.010 | Loss: 0.0784\n",
      "Episode 700/1500 | Reward: -3.40 | Average (10 ep): -3.20 | Length: 51 | Epsilon: 0.010 | Loss: 0.0552\n",
      "Episode 710/1500 | Reward: -0.90 | Average (10 ep): 0.45 | Length: 95 | Epsilon: 0.010 | Loss: 0.0575\n",
      "Episode 720/1500 | Reward: 7.20 | Average (10 ep): 1.05 | Length: 101 | Epsilon: 0.010 | Loss: 0.0631\n",
      "Model saved in best_flappy_model.pth\n",
      "Episode 730/1500 | Reward: -3.80 | Average (10 ep): -0.02 | Length: 61 | Epsilon: 0.010 | Loss: 0.0494\n",
      "Episode 740/1500 | Reward: 6.50 | Average (10 ep): 0.04 | Length: 135 | Epsilon: 0.010 | Loss: 0.0513\n",
      "Model saved in best_flappy_model.pth\n",
      "Episode 750/1500 | Reward: -7.00 | Average (10 ep): 1.58 | Length: 57 | Epsilon: 0.010 | Loss: 0.0553\n",
      "Model saved in best_flappy_model.pth\n",
      "Episode 760/1500 | Reward: 28.20 | Average (10 ep): 5.67 | Length: 281 | Epsilon: 0.010 | Loss: 0.0583\n",
      "Model saved in best_flappy_model.pth\n",
      "Episode 770/1500 | Reward: -1.50 | Average (10 ep): 1.66 | Length: 86 | Epsilon: 0.010 | Loss: 0.0677\n",
      "Episode 780/1500 | Reward: 2.20 | Average (10 ep): -0.03 | Length: 208 | Epsilon: 0.010 | Loss: 0.0717\n",
      "Model saved in best_flappy_model.pth\n",
      "Episode 790/1500 | Reward: -1.00 | Average (10 ep): 3.37 | Length: 123 | Epsilon: 0.010 | Loss: 0.0711\n",
      "Episode 800/1500 | Reward: 40.10 | Average (10 ep): 7.44 | Length: 291 | Epsilon: 0.010 | Loss: 0.0790\n",
      "Model saved in best_flappy_model.pth\n",
      "Episode 810/1500 | Reward: -5.30 | Average (10 ep): 5.70 | Length: 64 | Epsilon: 0.010 | Loss: 0.0829\n",
      "Episode 820/1500 | Reward: -4.20 | Average (10 ep): 9.41 | Length: 53 | Epsilon: 0.010 | Loss: 0.0826\n",
      "Model saved in best_flappy_model.pth\n",
      "Episode 830/1500 | Reward: -6.00 | Average (10 ep): 13.59 | Length: 50 | Epsilon: 0.010 | Loss: 0.0921\n",
      "Model saved in best_flappy_model.pth\n",
      "Episode 840/1500 | Reward: -6.00 | Average (10 ep): 11.80 | Length: 50 | Epsilon: 0.010 | Loss: 0.0646\n",
      "Episode 850/1500 | Reward: 4.80 | Average (10 ep): 15.25 | Length: 122 | Epsilon: 0.010 | Loss: 0.0830\n",
      "Episode 860/1500 | Reward: 16.90 | Average (10 ep): 15.07 | Length: 235 | Epsilon: 0.010 | Loss: 0.0940\n",
      "Model saved in best_flappy_model.pth\n",
      "Episode 870/1500 | Reward: 2.50 | Average (10 ep): 18.90 | Length: 91 | Epsilon: 0.010 | Loss: 0.1120\n",
      "Model saved in best_flappy_model.pth\n",
      "Episode 880/1500 | Reward: 10.90 | Average (10 ep): 25.65 | Length: 163 | Epsilon: 0.010 | Loss: 0.1114\n",
      "Episode 890/1500 | Reward: 19.00 | Average (10 ep): 16.23 | Length: 199 | Epsilon: 0.010 | Loss: 0.1063\n",
      "Episode 900/1500 | Reward: 65.20 | Average (10 ep): 26.61 | Length: 574 | Epsilon: 0.010 | Loss: 0.1145\n",
      "Episode 910/1500 | Reward: 6.00 | Average (10 ep): 11.93 | Length: 122 | Epsilon: 0.010 | Loss: 0.1141\n",
      "Episode 920/1500 | Reward: 61.80 | Average (10 ep): 24.09 | Length: 431 | Epsilon: 0.010 | Loss: 0.1211\n",
      "Model saved in best_flappy_model.pth\n",
      "Model saved in best_flappy_model.pth\n",
      "Episode 930/1500 | Reward: -1.40 | Average (10 ep): 34.64 | Length: 61 | Epsilon: 0.010 | Loss: 0.1193\n",
      "Episode 940/1500 | Reward: 26.50 | Average (10 ep): 31.20 | Length: 238 | Epsilon: 0.010 | Loss: 0.1266\n",
      "Episode 950/1500 | Reward: 9.40 | Average (10 ep): 40.12 | Length: 106 | Epsilon: 0.010 | Loss: 0.1024\n",
      "Episode 960/1500 | Reward: 51.60 | Average (10 ep): 30.13 | Length: 437 | Epsilon: 0.010 | Loss: 0.1339\n",
      "Episode 970/1500 | Reward: 19.00 | Average (10 ep): 19.20 | Length: 181 | Epsilon: 0.010 | Loss: 0.1419\n",
      "Episode 980/1500 | Reward: 40.90 | Average (10 ep): 36.55 | Length: 295 | Epsilon: 0.010 | Loss: 0.1185\n",
      "Episode 990/1500 | Reward: 96.30 | Average (10 ep): 57.77 | Length: 743 | Epsilon: 0.010 | Loss: 0.1229\n",
      "Episode 1000/1500 | Reward: 6.00 | Average (10 ep): 28.51 | Length: 104 | Epsilon: 0.010 | Loss: 0.1503\n",
      "Model saved in flappy_model_ep1000.pth\n",
      "Episode 1010/1500 | Reward: 22.30 | Average (10 ep): 26.58 | Length: 244 | Epsilon: 0.010 | Loss: 0.1303\n",
      "Episode 1020/1500 | Reward: 251.80 | Average (10 ep): 65.55 | Length: 1555 | Epsilon: 0.010 | Loss: 0.1331\n",
      "Model saved in best_flappy_model.pth\n",
      "Episode 1030/1500 | Reward: -6.00 | Average (10 ep): 22.89 | Length: 50 | Epsilon: 0.010 | Loss: 0.1212\n",
      "Episode 1040/1500 | Reward: 83.50 | Average (10 ep): 53.25 | Length: 619 | Epsilon: 0.010 | Loss: 0.1352\n",
      "Episode 1050/1500 | Reward: 85.70 | Average (10 ep): 62.93 | Length: 687 | Epsilon: 0.010 | Loss: 0.1534\n",
      "Episode 1060/1500 | Reward: 251.20 | Average (10 ep): 60.10 | Length: 1555 | Epsilon: 0.010 | Loss: 0.1377\n",
      "Episode 1070/1500 | Reward: 2.50 | Average (10 ep): 11.50 | Length: 70 | Epsilon: 0.010 | Loss: 0.1555\n",
      "Episode 1080/1500 | Reward: 21.70 | Average (10 ep): 55.80 | Length: 235 | Epsilon: 0.010 | Loss: 0.1369\n",
      "Episode 1090/1500 | Reward: -2.70 | Average (10 ep): 33.70 | Length: 86 | Epsilon: 0.010 | Loss: 0.1379\n",
      "Episode 1100/1500 | Reward: 118.90 | Average (10 ep): 54.67 | Length: 820 | Epsilon: 0.010 | Loss: 0.1252\n",
      "Episode 1110/1500 | Reward: 15.10 | Average (10 ep): 37.36 | Length: 163 | Epsilon: 0.010 | Loss: 0.1177\n",
      "Episode 1120/1500 | Reward: 1.50 | Average (10 ep): 28.47 | Length: 86 | Epsilon: 0.010 | Loss: 0.1149\n",
      "Episode 1130/1500 | Reward: 74.00 | Average (10 ep): 33.57 | Length: 555 | Epsilon: 0.010 | Loss: 0.1233\n",
      "Episode 1140/1500 | Reward: 23.10 | Average (10 ep): 26.81 | Length: 212 | Epsilon: 0.010 | Loss: 0.1374\n",
      "Episode 1150/1500 | Reward: 59.20 | Average (10 ep): 27.74 | Length: 406 | Epsilon: 0.010 | Loss: 0.1148\n",
      "Episode 1160/1500 | Reward: -2.00 | Average (10 ep): 29.42 | Length: 55 | Epsilon: 0.010 | Loss: 0.0981\n",
      "Episode 1170/1500 | Reward: 17.80 | Average (10 ep): 45.53 | Length: 199 | Epsilon: 0.010 | Loss: 0.1329\n",
      "Episode 1180/1500 | Reward: 24.40 | Average (10 ep): 30.53 | Length: 199 | Epsilon: 0.010 | Loss: 0.1016\n",
      "Episode 1190/1500 | Reward: 1.50 | Average (10 ep): 38.06 | Length: 68 | Epsilon: 0.010 | Loss: 0.0897\n",
      "Episode 1200/1500 | Reward: 7.00 | Average (10 ep): 30.25 | Length: 106 | Epsilon: 0.010 | Loss: 0.0858\n",
      "Episode 1210/1500 | Reward: 10.30 | Average (10 ep): 44.02 | Length: 139 | Epsilon: 0.010 | Loss: 0.0898\n",
      "Episode 1220/1500 | Reward: 65.80 | Average (10 ep): 46.36 | Length: 481 | Epsilon: 0.010 | Loss: 0.0910\n",
      "Episode 1230/1500 | Reward: -1.00 | Average (10 ep): 28.65 | Length: 57 | Epsilon: 0.010 | Loss: 0.0941\n",
      "Episode 1240/1500 | Reward: 60.90 | Average (10 ep): 22.38 | Length: 389 | Epsilon: 0.010 | Loss: 0.0912\n",
      "Episode 1250/1500 | Reward: 45.10 | Average (10 ep): 35.16 | Length: 292 | Epsilon: 0.010 | Loss: 0.0895\n",
      "Episode 1260/1500 | Reward: 20.20 | Average (10 ep): 29.43 | Length: 178 | Epsilon: 0.010 | Loss: 0.0880\n",
      "Model saved in best_flappy_model.pth\n",
      "Episode 1270/1500 | Reward: 5.50 | Average (10 ep): 55.48 | Length: 91 | Epsilon: 0.010 | Loss: 0.0868\n",
      "Episode 1280/1500 | Reward: 104.60 | Average (10 ep): 32.68 | Length: 735 | Epsilon: 0.010 | Loss: 0.0759\n",
      "Episode 1290/1500 | Reward: -3.70 | Average (10 ep): 51.04 | Length: 66 | Epsilon: 0.010 | Loss: 0.0789\n",
      "Episode 1300/1500 | Reward: 28.00 | Average (10 ep): 37.26 | Length: 199 | Epsilon: 0.010 | Loss: 0.0762\n",
      "Model saved in best_flappy_model.pth\n",
      "Episode 1310/1500 | Reward: 196.40 | Average (10 ep): 75.41 | Length: 1107 | Epsilon: 0.010 | Loss: 0.0754\n",
      "Episode 1320/1500 | Reward: 46.80 | Average (10 ep): 20.20 | Length: 332 | Epsilon: 0.010 | Loss: 0.0734\n",
      "Episode 1330/1500 | Reward: 20.10 | Average (10 ep): 28.97 | Length: 173 | Epsilon: 0.010 | Loss: 0.0852\n",
      "Episode 1340/1500 | Reward: 13.70 | Average (10 ep): 49.59 | Length: 135 | Epsilon: 0.010 | Loss: 0.0625\n",
      "Episode 1350/1500 | Reward: 67.30 | Average (10 ep): 31.11 | Length: 439 | Epsilon: 0.010 | Loss: 0.0691\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\\n\")\n",
    "\n",
    "# train the agent\n",
    "training_rewards = train_dqn(\n",
    "    agent=agent,\n",
    "    env=env,\n",
    "    n_episodes=HYPERPARAMETERS['n_episodes'],\n",
    "    max_steps=HYPERPARAMETERS['max_steps'],\n",
    "    save_freq=HYPERPARAMETERS['save_freq'],\n",
    "    print_freq=HYPERPARAMETERS['print_freq']\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed\")\n",
    "\n",
    "# visualize results\n",
    "plot_training_results(training_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "QZcN770ZuRhU",
    "outputId": "a03241b4-986e-4020-e11b-0df06e563c3e"
   },
   "outputs": [],
   "source": [
    "# evaluate the agent\n",
    "print(\"Evaluating the trained agent...\\n\")\n",
    "\n",
    "# load the best model\n",
    "agent.load('best_flappy_model.pth')\n",
    "\n",
    "# evalueate\n",
    "eval_rewards, eval_scores = evaluate_agent(agent=agent, env=env, n_episodes=20,render=False)\n",
    "\n",
    "# graph the results\n",
    "plot_evaluation_results(eval_scores)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
