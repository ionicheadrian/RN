{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T11:20:32.257032300Z",
     "start_time": "2026-01-11T10:52:54.617433800Z"
    },
    "id": "NIwT79NEu6qj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python-headless in c:\\users\\ionic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.12.0.88)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in c:\\users\\ionic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opencv-python-headless) (2.2.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install the necessary libraries\n",
    "# !pip install gym\n",
    "# !pip install pygame\n",
    "# !pip install opencv-python\n",
    "# !pip install flappy-bird-gymnasium\n",
    "# !pip install matplotlib\n",
    "# !pip install torch torchvision\n",
    "%pip install opencv-python-headless\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mNsP66_wvBbk"
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import random\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import flappy_bird_gymnasium\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "raAx8302vJ75",
    "outputId": "2bd78d85-a2ab-458c-af90-c88fde4ec766"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: cpu\n"
     ]
    }
   ],
   "source": [
    "# configure the device (GPU if its available if not, CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "eGoaiZhzvyhP"
   },
   "outputs": [],
   "source": [
    "# configure seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "gvKP690lvV7N"
   },
   "outputs": [],
   "source": [
    "# image preprocessing\n",
    "class ImagePreprocessor:\n",
    "  \"\"\"\n",
    "  class to preprocess the images from the flappy bird environment\n",
    "  converts images from rgb to grayscale, resizes and normalizes\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, width=84, height=84):\n",
    "    self.width = width\n",
    "    self.height = height\n",
    "\n",
    "  def preprocess(self, frame):\n",
    "    \"\"\"\n",
    "    preprocess a frame of the game\n",
    "    arguments:\n",
    "      frame: frame of the game (H, W, 3) or (H, W) if already grayscale\n",
    "    returns:\n",
    "      preprocessed frame (1, H, W) normalized\n",
    "    \"\"\"\n",
    "    # convert to grayscale (handle different input formats)\n",
    "    if len(frame.shape) == 2:\n",
    "      # already grayscale\n",
    "      gray = frame\n",
    "    elif len(frame.shape) == 3:\n",
    "      if frame.shape[2] == 1:\n",
    "        # already grayscale but with extra dimension\n",
    "        gray = frame[:, :, 0]\n",
    "      elif frame.shape[2] == 3:\n",
    "        # convert rgb to grayscale\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "      elif frame.shape[2] == 4:\n",
    "        # convert rgba to grayscale\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_RGBA2GRAY)\n",
    "      else:\n",
    "        raise ValueError(f\"unsupported number of channels: {frame.shape[2]}\")\n",
    "    else:\n",
    "      raise ValueError(f\"unsupported frame shape: {frame.shape}\")\n",
    "\n",
    "    # resize\n",
    "    resized = cv2.resize(gray, (self.width, self.height),\n",
    "                        interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # normalize (values between 0 and 1)\n",
    "    normalized = resized.astype(np.float32) / 255.0\n",
    "\n",
    "    # add a channel dimension (H, W) -> (1, H, W)\n",
    "    preprocessed = np.expand_dims(normalized, axis=0)\n",
    "\n",
    "    return preprocessed\n",
    "\n",
    "  def preprocess_batch(self, frames):\n",
    "    \"\"\"preprocesses a batch of frames\"\"\"\n",
    "    return np.array([self.preprocess(frame) for frame in frames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xwHf1RVzw4zi",
    "outputId": "1a5cf11c-ca9e-47fa-f2a8-456ca1bfefdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN model created\n",
      "Number of parameters: 1,685,154\n"
     ]
    }
   ],
   "source": [
    "# convolutional neural network\n",
    "class DQN(nn.Module):\n",
    "  \"\"\"\n",
    "  Deep Q-Network with a convolutional architecture to process imaghes.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, input_channels=4, n_actions=2):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      input_channels: number of stacked frames (to capture movement)\n",
    "      n_actions: number of possible actiones (0=do nothing, 1=jump)\n",
    "    \"\"\"\n",
    "    super(DQN, self).__init__()\n",
    "\n",
    "    # convolutional layers\n",
    "    self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=8, stride=4)\n",
    "    self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "    self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "\n",
    "    # compute the szie of the output of the convolutional layers\n",
    "    # for images 84x84\n",
    "    # after conv1:(84-8)/4 + 1 = 20\n",
    "    # after conv2:(20-4)/2 + 1 = 9\n",
    "    # after conv3:(9-3)/1 + 1 = 7\n",
    "    # total: 64 * 7 * 7 = 3136\n",
    "\n",
    "    conv_output_size = 64 * 7 * 7\n",
    "\n",
    "    # fully connected layers\n",
    "    self.fc1 = nn.Linear(conv_output_size, 512)\n",
    "    self.fc2 = nn.Linear(512, n_actions)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      x: input tensor (batch_size, channels, height, width)\n",
    "    returns:\n",
    "      Q-values for each action (batch_size, n_actions)\n",
    "    \"\"\"\n",
    "    # convolutional layers with relu\n",
    "    x = F.relu(self.conv1(x))\n",
    "    x = F.relu(self.conv2(x))\n",
    "    x = F.relu(self.conv3(x))\n",
    "\n",
    "    # flatten for fully connected layers\n",
    "    x = x.view(x.size(0), -1)\n",
    "\n",
    "    # fully connected layers\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = self.fc2(x)  # Sin activación en la salida (Q-values pueden ser cualquier número)\n",
    "\n",
    "    return x\n",
    "\n",
    "# create a test instance\n",
    "test_model = DQN(input_channels=4, n_actions=2).to(device)\n",
    "print(f\"DQN model created\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in test_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "wMC2w6WbyNWE"
   },
   "outputs": [],
   "source": [
    "# replay buffer\n",
    "class ReplayBuffer:\n",
    "  \"\"\"\n",
    "  Memory buffer to store experience tuples (transitions) and sample mini-batches\n",
    "  Implements the concept of experience replay from deepmind\n",
    "  \"\"\"\n",
    "  def __init__(self, capacity=100000):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      capacity: maximum buffer size\n",
    "    \"\"\"\n",
    "    self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "  def push(self, state, action, reward, next_state, done):\n",
    "    \"\"\"\n",
    "    adds a transition to the buffer\n",
    "    arguments:\n",
    "        state: current state (array)\n",
    "        action: action taken (int)\n",
    "        reward: recieved rewars (float)\n",
    "        next_state: next state (numpy array)\n",
    "        done: if the episode finished (bool)\n",
    "    \"\"\"\n",
    "    self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "  def sample(self, batch_size):\n",
    "    \"\"\"\n",
    "    samples a random minibatch from the buffer\n",
    "    arguments:\n",
    "      batch_size: batch size\n",
    "    returns:\n",
    "      tensors tuples (states, actions, rewards, next_states, dones)\n",
    "    \"\"\"\n",
    "    # random sampling\n",
    "    batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "    # separate the components\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "    # convert to tensors\n",
    "    states = torch.FloatTensor(np.array(states)).to(device)\n",
    "    actions = torch.LongTensor(actions).to(device)\n",
    "    rewards = torch.FloatTensor(rewards).to(device)\n",
    "    next_states = torch.FloatTensor(np.array(next_states)).to(device)\n",
    "    dones = torch.FloatTensor(dones).to(device)\n",
    "\n",
    "    return states, actions, rewards, next_states, dones\n",
    "\n",
    "  def __len__(self):\n",
    "    \"\"\"returns the current size of the buffer\"\"\"\n",
    "    return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "9N5rFPwh0Fox"
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "  \"\"\"\n",
    "  agent that implements the q-learning algorithm using a deep neural network\n",
    "  \"\"\"\n",
    "  def __init__(self, state_shape=(4, 84, 84), n_actions=2, learning_rate=0.00025, gamma=0.99, epsilon_start=1.0,\n",
    "               epsilon_end=0.01, epsilon_decay=0.995,buffer_capacity=100000, batch_size=32, target_update_freq=1000):\n",
    "\n",
    "    self.n_actions = n_actions\n",
    "    self.gamma = gamma\n",
    "    self.epsilon = epsilon_start\n",
    "    self.epsilon_end = epsilon_end\n",
    "    self.epsilon_decay = epsilon_decay\n",
    "    self.batch_size = batch_size\n",
    "    self.target_update_freq = target_update_freq\n",
    "    self.steps = 0\n",
    "\n",
    "    # neural networks (policy net and target net)\n",
    "    self.policy_net = DQN(state_shape[0], n_actions).to(device)\n",
    "    self.target_net = DQN(state_shape[0], n_actions).to(device)\n",
    "\n",
    "    # copy the initial weights to the target network\n",
    "    self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "    self.target_net.eval()  # target netwotk in evaluation mode\n",
    "\n",
    "    # adam optimizer and huber loss\n",
    "    self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)\n",
    "    self.criterion = nn.SmoothL1Loss()\n",
    "\n",
    "    # replay buffer\n",
    "    self.memory = ReplayBuffer(buffer_capacity)\n",
    "\n",
    "    # metrics\n",
    "    self.losses = []\n",
    "    self.rewards_history = []\n",
    "\n",
    "  def select_action(self, state, training=True):\n",
    "    \"\"\"\n",
    "    selects an action using the epsilon greedy strategy\n",
    "    arguments:\n",
    "      state: current state (numpy array)\n",
    "      training: if its in training mode (uses epsilon-greedy)\n",
    "    returns:\n",
    "      selectd action (int)\n",
    "    \"\"\"\n",
    "    if training and random.random() < self.epsilon:\n",
    "      # exploration: choose a random action\n",
    "      return random.randrange(self.n_actions)\n",
    "    else:\n",
    "      # explotation: choose the action with the best Q-value\n",
    "      with torch.no_grad():\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        q_values = self.policy_net(state_tensor)\n",
    "        return q_values.argmax(1).item()\n",
    "\n",
    "  def store_transition(self, state, action, reward, next_state, done):\n",
    "    \"\"\"stores a transition in the replay buffer\"\"\"\n",
    "    self.memory.push(state, action, reward, next_state, done)\n",
    "\n",
    "  def update(self):\n",
    "    \"\"\"\n",
    "    updates the neural network using Q-learning\n",
    "    implements the Bellman equation: Q(s,a) = r + γ * max(Q(s',a'))))\n",
    "    \"\"\"\n",
    "    # verify if there are enough experiences\n",
    "    if len(self.memory) < self.batch_size:\n",
    "      return None\n",
    "\n",
    "    # sample a mini-batch\n",
    "    states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "\n",
    "    # calculate current Q-values Q(s,a)\n",
    "    current_q_values = self.policy_net(states).gather(1, actions.unsqueeze(1))\n",
    "\n",
    "    # calculate goal Q-values r + γ * max(Q(s',a'))\n",
    "    with torch.no_grad():\n",
    "      next_q_values = self.target_net(next_states).max(1)[0]\n",
    "      # if the episode finished (done=1), future value is 0\n",
    "      target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "    # calculate the loss (current Q - goal Q)\n",
    "    loss = self.criterion(current_q_values.squeeze(), target_q_values)\n",
    "\n",
    "    # optimization\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # gradient clipping for stability\n",
    "    torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 10)\n",
    "\n",
    "    self.optimizer.step()\n",
    "\n",
    "    # save the losses\n",
    "    self.losses.append(loss.item())\n",
    "\n",
    "    # update the target network periodically\n",
    "    self.steps += 1\n",
    "    if self.steps % self.target_update_freq == 0:\n",
    "      self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "  def decay_epsilon(self):\n",
    "    \"\"\"reduces epsiolon to explore less with time\"\"\"\n",
    "    self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "  def save(self, filepath):\n",
    "    \"\"\"saves the trained model\"\"\"\n",
    "    torch.save({\n",
    "      'policy_net_state_dict': self.policy_net.state_dict(),\n",
    "      'target_net_state_dict': self.target_net.state_dict(),\n",
    "      'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "      'epsilon': self.epsilon,\n",
    "      'steps': self.steps\n",
    "    }, filepath)\n",
    "    print(f\"Model saved in {filepath}\")\n",
    "\n",
    "  def load(self, filepath):\n",
    "    \"\"\"loads a trained model\"\"\"\n",
    "    checkpoint = torch.load(filepath, map_location=device)\n",
    "    self.policy_net.load_state_dict(checkpoint['policy_net_state_dict'])\n",
    "    self.target_net.load_state_dict(checkpoint['target_net_state_dict'])\n",
    "    self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    self.epsilon = checkpoint['epsilon']\n",
    "    self.steps = checkpoint['steps']\n",
    "    print(f\"Model loaded from {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "z1jz7C5WULn-"
   },
   "outputs": [],
   "source": [
    "  # environment wrapper\n",
    "class FlappyBirdWrapper:\n",
    "    \"\"\"\n",
    "    wrapper for the flappy bird environment that includes:\n",
    "    - image preprocessing\n",
    "    - frame stacking (stack frames to capture movement)\n",
    "    - personalized rewards\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_frames=4):\n",
    "      \"\"\"\n",
    "      arguments:\n",
    "        n_frames: number of consective frames to stack\n",
    "      \"\"\"\n",
    "      # create the environment with render mode to get pixels\n",
    "      self.env = gym.make(\"FlappyBird-v0\", render_mode=\"rgb_array\")\n",
    "      self.n_frames = n_frames\n",
    "      self.preprocessor = ImagePreprocessor(84, 84)\n",
    "\n",
    "      # frame buffer\n",
    "      self.frame_buffer = deque(maxlen=n_frames)\n",
    "\n",
    "      # step counter\n",
    "      self.total_steps = 0\n",
    "\n",
    "    def reset(self):\n",
    "      \"\"\"\n",
    "      resets the environment\n",
    "      returns:\n",
    "        initial state (stacked frames)\n",
    "      \"\"\"\n",
    "      obs, _ = self.env.reset()\n",
    "\n",
    "      # get the visual frame (render) instead of the vectorial observation\n",
    "      visual_frame = self.env.render()\n",
    "\n",
    "      # preprocess the initial frame\n",
    "      processed_frame = self.preprocessor.preprocess(visual_frame)\n",
    "\n",
    "      # fill the buffer with the same initial frame\n",
    "      for _ in range(self.n_frames):\n",
    "        self.frame_buffer.append(processed_frame)\n",
    "      self.total_steps = 0\n",
    "\n",
    "      return self._get_state()\n",
    "\n",
    "    def step(self, action):\n",
    "      \"\"\"\n",
    "      exectutes an action in the environment\n",
    "\n",
    "      arguments:\n",
    "        action: action to execute (0 o 1)\n",
    "      returns:\n",
    "        tuple (state, reward, done, info)\n",
    "      \"\"\"\n",
    "      obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "      done = terminated or truncated\n",
    "\n",
    "      # get the visual frame (render) instead of the vectorial observation\n",
    "      visual_frame = self.env.render()\n",
    "\n",
    "      # preprocess the frame\n",
    "      processed_frame = self.preprocessor.preprocess(visual_frame)\n",
    "\n",
    "      # updatae the buffer\n",
    "      self.frame_buffer.append(processed_frame)\n",
    "\n",
    "      # personalized reward\n",
    "      # penalize diying and reward surviving\n",
    "      if done:\n",
    "          custom_reward = -10  # penalization for dying\n",
    "      else:\n",
    "          custom_reward = 0.1  # small reward for surviving\n",
    "\n",
    "      # use the games original reward (go though the tube) + the personalized reward\n",
    "      total_reward = reward + custom_reward\n",
    "\n",
    "      self.total_steps += 1\n",
    "\n",
    "      return self._get_state(), total_reward, done, info\n",
    "\n",
    "    def _get_state(self):\n",
    "        \"\"\"\n",
    "        obtains the current state (stacked frmaes).\n",
    "        returns:\n",
    "          array of stacked frames (n_frames, height, width)\n",
    "        \"\"\"\n",
    "        return np.concatenate(list(self.frame_buffer), axis=0)\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"renders the environment\"\"\"\n",
    "        return self.env.render()\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"closes the environment\"\"\"\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "QCe5nhUgWj-A"
   },
   "outputs": [],
   "source": [
    "# training function\n",
    "def train_dqn(agent, env, n_episodes=1000, max_steps=10000, save_freq=100, print_freq=10):\n",
    "  \"\"\"\n",
    "  trains the DQN agent in the flappy bird environment\n",
    "  arguments:\n",
    "    agent: DQN agent\n",
    "    env: environment wrapper\n",
    "    n_episodes: number of training episodes\n",
    "    max_steps: max number of steps per episode\n",
    "    save_freq: saving frequence of the model\n",
    "    print_freq: stats printing sequence\n",
    "  returns:\n",
    "    list of the rewards for each episode\n",
    "  \"\"\"\n",
    "  episode_rewards = []\n",
    "  episode_lengths = []\n",
    "  best_reward = -float('inf')\n",
    "\n",
    "  for episode in range(1, n_episodes + 1):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_loss = []\n",
    "\n",
    "    for step in range(max_steps):\n",
    "      # Seleccionar acción\n",
    "      action = agent.select_action(state, training=True)\n",
    "\n",
    "      # Ejecutar acción\n",
    "      next_state, reward, done, info = env.step(action)\n",
    "\n",
    "      # Almacenar transición\n",
    "      agent.store_transition(state, action, reward, next_state, done)\n",
    "\n",
    "      # Actualizar red neuronal\n",
    "      loss = agent.update()\n",
    "      if loss is not None:\n",
    "        episode_loss.append(loss)\n",
    "\n",
    "      episode_reward += reward\n",
    "      state = next_state\n",
    "\n",
    "      if done:\n",
    "        break\n",
    "\n",
    "    # decay epsilon\n",
    "    agent.decay_epsilon()\n",
    "\n",
    "    # save the metrics\n",
    "    episode_rewards.append(episode_reward)\n",
    "    episode_lengths.append(step + 1)\n",
    "\n",
    "    # print the progress\n",
    "    if episode % print_freq == 0:\n",
    "      avg_reward = np.mean(episode_rewards[-print_freq:])\n",
    "      avg_length = np.mean(episode_lengths[-print_freq:])\n",
    "      avg_loss = np.mean(episode_loss) if episode_loss else 0\n",
    "\n",
    "      print(f\"Episode {episode}/{n_episodes} | \"\n",
    "            f\"Reward: {episode_reward:.2f} | \"\n",
    "            f\"Average ({print_freq} ep): {avg_reward:.2f} | \"\n",
    "            f\"Length: {step+1} | \"\n",
    "            f\"Epsilon: {agent.epsilon:.3f} | \"\n",
    "            f\"Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # save the best model\n",
    "    if episode_reward > best_reward:\n",
    "      best_reward = episode_reward\n",
    "      agent.save('best_flappy_model.pth')\n",
    "\n",
    "    # save the model periodically\n",
    "    if episode % save_freq == 0:\n",
    "      agent.save(f'flappy_model_ep{episode}.pth')\n",
    "\n",
    "  return episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "p2Z7KbVnWoVI"
   },
   "outputs": [],
   "source": [
    "# evaluation function\n",
    "def evaluate_agent(agent, env, n_episodes=10, render=False):\n",
    "  \"\"\"\n",
    "  evaluates the performance of the agent trained without exploration\n",
    "  arguments:\n",
    "    agent: trained DQN agent\n",
    "    env: environment wrapper\n",
    "    n_episodes: number of evaluation episodes\n",
    "    render: if the game renders\n",
    "  returns:\n",
    "    list of the rewards and scores per episode\n",
    "  \"\"\"\n",
    "  episode_rewards = []\n",
    "  episode_scores = []\n",
    "\n",
    "  for episode in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    score = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "      # select the best action (without exploration)\n",
    "      action = agent.select_action(state, training=False)\n",
    "\n",
    "      # execute action\n",
    "      state, reward, done, info = env.step(action)\n",
    "      episode_reward += reward\n",
    "\n",
    "      # the score is usually the number of tubes gone through\n",
    "      if 'score' in info:\n",
    "        score = info['score']\n",
    "\n",
    "      if render:\n",
    "        frame = env.render()\n",
    "        plt.imshow(frame)\n",
    "        plt.axis('off')\n",
    "        plt.pause(0.01)\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "      episode_rewards.append(episode_reward)\n",
    "      episode_scores.append(score)\n",
    "\n",
    "      print(f\"Episode {episode+1}/{n_episodes} | \"\n",
    "            f\"Reward: {episode_reward:.2f} | \"\n",
    "            f\"Score: {score}\")\n",
    "\n",
    "  print(f\"\\nEvaluation results:\")\n",
    "  print(f\"Average reward: {np.mean(episode_rewards):.2f} ± {np.std(episode_rewards):.2f}\")\n",
    "  print(f\"Average score: {np.mean(episode_scores):.2f} ± {np.std(episode_scores):.2f}\")\n",
    "  print(f\"Best score: {np.max(episode_scores)}\")\n",
    "\n",
    "  return episode_rewards, episode_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Sa56QJyAWt23"
   },
   "outputs": [],
   "source": [
    "# graphing functions\n",
    "def plot_training_results(rewards, window=50):\n",
    "  \"\"\"\n",
    "  plots the results of the training\n",
    "  arguments:\n",
    "    rewards: list of rewards per episode\n",
    "    window: size of the moving average window\n",
    "  \"\"\"\n",
    "  plt.figure(figsize=(12, 5))\n",
    "\n",
    "  # rewards per epsiode\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plt.plot(rewards, alpha=0.3, label='Reward per episode')\n",
    "\n",
    "  # moving average\n",
    "  if len(rewards) >= window:\n",
    "      moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "      plt.plot(range(window-1, len(rewards)), moving_avg,\n",
    "              label=f'Moving average ({window} ep)', linewidth=2)\n",
    "\n",
    "  plt.xlabel('Episode')\n",
    "  plt.ylabel('Reward')\n",
    "  plt.title('Rewards during training')\n",
    "  plt.legend()\n",
    "  plt.grid(True, alpha=0.3)\n",
    "\n",
    "  # rewards histogram\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plt.hist(rewards, bins=30, edgecolor='black', alpha=0.7)\n",
    "  plt.xlabel('Reward')\n",
    "  plt.ylabel('Frequence')\n",
    "  plt.title('Rewards distribution')\n",
    "  plt.grid(True, alpha=0.3)\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.savefig('training_results.png', dpi=150, bbox_inches='tight')\n",
    "  plt.show()\n",
    "\n",
    "  print(\"Plot saved as 'training_results.png'\")\n",
    "\n",
    "def plot_evaluation_results(scores):\n",
    "  \"\"\"\n",
    "  plots the results of evaluation\n",
    "  arguments:\n",
    "    scores: list of the evaluation scores\n",
    "  \"\"\"\n",
    "  plt.figure(figsize=(10, 5))\n",
    "\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plt.bar(range(1, len(scores)+1), scores, color='skyblue', edgecolor='black')\n",
    "  plt.xlabel('Episode')\n",
    "  plt.ylabel('Score')\n",
    "  plt.title('Evaluation Scores')\n",
    "  plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plt.boxplot(scores)\n",
    "  plt.ylabel('Score')\n",
    "  plt.title('Scores distribution')\n",
    "  plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.savefig('evaluation_results.png', dpi=150, bbox_inches='tight')\n",
    "  plt.show()\n",
    "\n",
    "  print(\"Plot saved as 'evaluation_results.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GN53SZHuWzqK",
    "outputId": "f31abee0-adb3-46fc-c441-330bba9ecccf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Flappy Bird environment...\n",
      "Initializing DQN agent...\n",
      "\n",
      "Ready to train the DQN agent\n"
     ]
    }
   ],
   "source": [
    "# configuration an initialization\n",
    "\n",
    "# training hyperparameters\n",
    "HYPERPARAMETERS = {\n",
    "    'n_episodes': 500,\n",
    "    'learning_rate': 0.00025,\n",
    "    'gamma': 0.99, # discount factor\n",
    "    'epsilon_start': 1.0,\n",
    "    'epsilon_end': 0.01,\n",
    "    'epsilon_decay': 0.995,\n",
    "    'buffer_capacity': 100000,\n",
    "    'batch_size': 32,\n",
    "    'target_update_freq': 1000,\n",
    "    'n_frames': 4,\n",
    "    'max_steps': 10000,\n",
    "    'save_freq': 50,\n",
    "    'print_freq': 10\n",
    "}\n",
    "\n",
    "# create environment\n",
    "print(\"Creating Flappy Bird environment...\")\n",
    "env = FlappyBirdWrapper(n_frames=HYPERPARAMETERS['n_frames'])\n",
    "\n",
    "# create agent\n",
    "print(\"Initializing DQN agent...\")\n",
    "agent = DQNAgent(\n",
    "    state_shape=(HYPERPARAMETERS['n_frames'], 84, 84),\n",
    "    n_actions=2,\n",
    "    learning_rate=HYPERPARAMETERS['learning_rate'],\n",
    "    gamma=HYPERPARAMETERS['gamma'],\n",
    "    epsilon_start=HYPERPARAMETERS['epsilon_start'],\n",
    "    epsilon_end=HYPERPARAMETERS['epsilon_end'],\n",
    "    epsilon_decay=HYPERPARAMETERS['epsilon_decay'],\n",
    "    buffer_capacity=HYPERPARAMETERS['buffer_capacity'],\n",
    "    batch_size=HYPERPARAMETERS['batch_size'],\n",
    "    target_update_freq=HYPERPARAMETERS['target_update_freq']\n",
    ")\n",
    "\n",
    "print(\"\\nReady to train the DQN agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Gly4ebRPb5Rz",
    "outputId": "cf3fbe87-c6fc-4e79-b456-d7b1d337f290"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ionic\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in best_flappy_model.pth\n",
      "Model saved in best_flappy_model.pth\n",
      "Episode 10/500 | Reward: -12.60 | Average (10 ep): -12.66 | Length: 50 | Epsilon: 0.951 | Loss: 0.0052\n",
      "Episode 20/500 | Reward: -11.40 | Average (10 ep): -12.72 | Length: 50 | Epsilon: 0.905 | Loss: 0.0040\n",
      "Model saved in best_flappy_model.pth\n",
      "Episode 30/500 | Reward: -13.20 | Average (10 ep): -12.30 | Length: 50 | Epsilon: 0.860 | Loss: 0.0041\n",
      "Model saved in best_flappy_model.pth\n",
      "Episode 40/500 | Reward: -11.40 | Average (10 ep): -12.18 | Length: 50 | Epsilon: 0.818 | Loss: 0.0070\n",
      "Episode 50/500 | Reward: -12.00 | Average (10 ep): -13.02 | Length: 50 | Epsilon: 0.778 | Loss: 0.0044\n",
      "Model saved in flappy_model_ep50.pth\n",
      "Episode 60/500 | Reward: -13.20 | Average (10 ep): -12.30 | Length: 50 | Epsilon: 0.740 | Loss: 0.0044\n",
      "Model saved in best_flappy_model.pth\n",
      "Episode 70/500 | Reward: -13.20 | Average (10 ep): -12.18 | Length: 50 | Epsilon: 0.704 | Loss: 0.0052\n",
      "Episode 80/500 | Reward: -10.80 | Average (10 ep): -12.18 | Length: 50 | Epsilon: 0.670 | Loss: 0.0054\n",
      "Episode 90/500 | Reward: -13.20 | Average (10 ep): -12.00 | Length: 50 | Epsilon: 0.637 | Loss: 0.0054\n",
      "Episode 100/500 | Reward: -12.60 | Average (10 ep): -12.48 | Length: 50 | Epsilon: 0.606 | Loss: 0.0046\n",
      "Model saved in flappy_model_ep100.pth\n",
      "Episode 110/500 | Reward: -12.00 | Average (10 ep): -12.48 | Length: 50 | Epsilon: 0.576 | Loss: 0.0066\n",
      "Episode 120/500 | Reward: -12.00 | Average (10 ep): -12.12 | Length: 50 | Epsilon: 0.548 | Loss: 0.0083\n",
      "Episode 130/500 | Reward: -10.80 | Average (10 ep): -12.18 | Length: 50 | Epsilon: 0.521 | Loss: 0.0049\n",
      "Model saved in best_flappy_model.pth\n",
      "Episode 140/500 | Reward: -7.80 | Average (10 ep): -10.50 | Length: 50 | Epsilon: 0.496 | Loss: 0.0060\n",
      "Episode 150/500 | Reward: -12.60 | Average (10 ep): -12.36 | Length: 50 | Epsilon: 0.471 | Loss: 0.0078\n",
      "Model saved in flappy_model_ep150.pth\n",
      "Episode 160/500 | Reward: -12.60 | Average (10 ep): -11.52 | Length: 50 | Epsilon: 0.448 | Loss: 0.0048\n",
      "Episode 170/500 | Reward: -12.60 | Average (10 ep): -11.46 | Length: 50 | Epsilon: 0.427 | Loss: 0.0052\n",
      "Episode 180/500 | Reward: -10.20 | Average (10 ep): -10.08 | Length: 50 | Epsilon: 0.406 | Loss: 0.0065\n",
      "Episode 190/500 | Reward: -12.60 | Average (10 ep): -11.52 | Length: 50 | Epsilon: 0.386 | Loss: 0.0056\n",
      "Episode 200/500 | Reward: -9.60 | Average (10 ep): -10.44 | Length: 50 | Epsilon: 0.367 | Loss: 0.0073\n",
      "Model saved in flappy_model_ep200.pth\n",
      "Episode 210/500 | Reward: -13.20 | Average (10 ep): -10.74 | Length: 50 | Epsilon: 0.349 | Loss: 0.0077\n",
      "Episode 220/500 | Reward: -11.40 | Average (10 ep): -10.98 | Length: 50 | Epsilon: 0.332 | Loss: 0.0077\n",
      "Episode 230/500 | Reward: -13.80 | Average (10 ep): -11.94 | Length: 50 | Epsilon: 0.316 | Loss: 0.0079\n",
      "Episode 240/500 | Reward: -13.80 | Average (10 ep): -11.52 | Length: 50 | Epsilon: 0.300 | Loss: 0.0049\n",
      "Episode 250/500 | Reward: -7.80 | Average (10 ep): -9.30 | Length: 50 | Epsilon: 0.286 | Loss: 0.0089\n",
      "Model saved in flappy_model_ep250.pth\n",
      "Episode 260/500 | Reward: -12.00 | Average (10 ep): -10.86 | Length: 50 | Epsilon: 0.272 | Loss: 0.0093\n",
      "Episode 270/500 | Reward: -12.60 | Average (10 ep): -9.12 | Length: 50 | Epsilon: 0.258 | Loss: 0.0089\n",
      "Episode 280/500 | Reward: -10.20 | Average (10 ep): -9.78 | Length: 50 | Epsilon: 0.246 | Loss: 0.0107\n",
      "Episode 290/500 | Reward: -9.60 | Average (10 ep): -9.18 | Length: 50 | Epsilon: 0.234 | Loss: 0.0089\n",
      "Episode 300/500 | Reward: -8.40 | Average (10 ep): -9.42 | Length: 50 | Epsilon: 0.222 | Loss: 0.0072\n",
      "Model saved in flappy_model_ep300.pth\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\\n\")\n",
    "\n",
    "# train the agent\n",
    "training_rewards = train_dqn(\n",
    "    agent=agent,\n",
    "    env=env,\n",
    "    n_episodes=HYPERPARAMETERS['n_episodes'],\n",
    "    max_steps=HYPERPARAMETERS['max_steps'],\n",
    "    save_freq=HYPERPARAMETERS['save_freq'],\n",
    "    print_freq=HYPERPARAMETERS['print_freq']\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed\")\n",
    "\n",
    "# visualize results\n",
    "plot_training_results(training_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "QZcN770ZuRhU",
    "outputId": "a03241b4-986e-4020-e11b-0df06e563c3e"
   },
   "outputs": [],
   "source": [
    "# evaluate the agent\n",
    "print(\"Evaluating the trained agent...\\n\")\n",
    "\n",
    "# load the best model\n",
    "agent.load('best_flappy_model.pth')\n",
    "\n",
    "# evalueate\n",
    "eval_rewards, eval_scores = evaluate_agent(agent=agent, env=env, n_episodes=20,render=False)\n",
    "\n",
    "# graph the results\n",
    "plot_evaluation_results(eval_scores)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
