{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports complete\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "np.random.seed(74)\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_file = \"../input/extended_mnist_train.pkl\"\n",
    "test_file = \"../input/extended_mnist_test.pkl\"\n",
    "\n",
    "with open(train_file, \"rb\") as fp:\n",
    "    train = pickle.load(fp)\n",
    "with open(test_file, \"rb\") as fp:\n",
    "    test = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "train_data = []\n",
    "train_labels = []\n",
    "for image, label in train:\n",
    "    train_data.append(image.flatten() / 255.0)\n",
    "    train_labels.append(label)\n",
    "\n",
    "test_data = []\n",
    "for image, label in test:\n",
    "    test_data.append(image.flatten() / 255.0)\n",
    "\n",
    "X_train_full = np.array(train_data)\n",
    "y_train_full = np.array(train_labels)\n",
    "X_test = np.array(test_data)\n",
    "\n",
    "# Split\n",
    "split = int(0.9 * len(X_train_full))\n",
    "X_train = X_train_full[:split]\n",
    "y_train = y_train_full[:split]\n",
    "X_val = X_train_full[split:]\n",
    "y_val = y_train_full[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    z = np.clip(z, -500, 500)\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivat(a):\n",
    "    return a * (1.0 - a)\n",
    "\n",
    "def one_hot_encode(y, n_classes=10):\n",
    "    n = len(y)\n",
    "    one_hot = np.zeros((n, n_classes))\n",
    "    one_hot[np.arange(n), y] = 1\n",
    "    return one_hot\n",
    "\n",
    "def softmax(z):\n",
    "    z = z - np.max(z, axis=1, keepdims=True)\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(a2, y, W1, W2, l2_lambda=0.0):\n",
    "    m = a2.shape[0]\n",
    "    y_encoded = one_hot_encode(y, a2.shape[1])\n",
    "    eps = 1e-12\n",
    "    loss = -np.sum(y_encoded * np.log(a2 + eps)) / m\n",
    "\n",
    "    if l2_lambda and l2_lambda > 0:\n",
    "        loss += 0.5 * l2_lambda * (np.sum(W1**2) + np.sum(W2**2))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(input_size, hidden_size, output_size):\n",
    "    \n",
    "    W1 = np.random.randn(input_size, hidden_size) * np.sqrt(1.0 / input_size)\n",
    "    b1 = np.zeros((1, hidden_size))\n",
    "    W2 = np.random.randn(hidden_size, output_size) * np.sqrt(1.0 / hidden_size)\n",
    "    b2 = np.zeros((1, output_size))\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def forward(X, W1, b1, W2, b2, dropout_rate=0, training=True):\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    \n",
    "\n",
    "    #aplicam un dropout pe stratu 1\n",
    "    dropout_mask = None\n",
    "    if training and dropout_rate > 0: # aici verificam daca suntem in starea de training \n",
    "        dropout_mask = (np.random.rand(*a1.shape) > dropout_rate) / (1 - dropout_rate)\n",
    "        a1 = a1 * dropout_mask\n",
    "    \n",
    "\n",
    "    #calculeaza output ul de pe stratul 2\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = softmax(z2)\n",
    "    \n",
    "    return z1, a1, z2, a2, dropout_mask\n",
    "\n",
    "def backward(X, y, z1, a1, z2, a2, W1, W2, dropout_mask, l2_lambda=0.0001):\n",
    "    m = X.shape[0]\n",
    "    y_encoded = one_hot_encode(y, a2.shape[1])\n",
    "    \n",
    "    delta2 = a2 - y_encoded #diferenta dintre ce am prezis si ce trebuia prezis\n",
    "\n",
    "    #regularizarea l2 pentru gradienti folosind stratu 2\n",
    "    dW2 = np.dot(a1.T, delta2) / m + l2_lambda * W2\n",
    "    db2 = np.sum(delta2, axis=0, keepdims=True) / m\n",
    "    \n",
    "\n",
    "    #propagam eroarea la primul strat \n",
    "    delta1 = np.dot(delta2, W2.T) * sigmoid_derivat(a1)\n",
    "    if dropout_mask is not None:\n",
    "        delta1 = delta1 * dropout_mask #aplicand si dropout ul\n",
    "\n",
    "    #same si aici regularizarea l2 folosind stratu 1\n",
    "    dW1 = np.dot(X.T, delta1) / m + l2_lambda * W1\n",
    "    db1 = np.sum(delta1, axis=0, keepdims=True) / m\n",
    "    \n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "def compute_accuracy(X, y, W1, b1, W2, b2):\n",
    "    _, _, _, a2, _ = forward(X, W1, b1, W2, b2, dropout_rate=0, training=False)\n",
    "    predictions = np.argmax(a2, axis=1)\n",
    "    return np.mean(predictions == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparametri\n",
    "HIDDEN_SIZE = 100  # conform cerintei\n",
    "EPOCHS = 60\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.01  \n",
    "DROPOUT_RATE = 0.3   # 30% din neuroni sunt inchisi \n",
    "L2_LAMBDA = 0.0001   #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incepem training cu 54000 si folosim 128 per epoca\n",
      "784 (28x28) -> 100 (marimea hidden layer ului -> 10\n",
      "folosim pentur regularizare : dropout=0.3 si L2=0.0001\n",
      "epoca   1/60 | train acc: 0.6178  val acc: 0.6472 | train loss: 2.2438  val loss: 2.0715 | lr: 0.0100  time: 1.2s\n",
      "noua acuratete de validare: 0.6472\n",
      "epoca   6/60 | train acc: 0.8040  val acc: 0.8428 | train loss: 1.1823  val loss: 1.0075 | lr: 0.0100  time: 6.3s\n",
      "noua acuratete de validare: 0.8428\n",
      "epoca  11/60 | train acc: 0.8508  val acc: 0.8850 | train loss: 0.8054  val loss: 0.6458 | lr: 0.0100  time: 11.4s\n",
      "noua acuratete de validare: 0.8850\n",
      "epoca  16/60 | train acc: 0.8701  val acc: 0.8998 | train loss: 0.6575  val loss: 0.5031 | lr: 0.0100  time: 16.5s\n",
      "noua acuratete de validare: 0.8998\n",
      "epoca  21/60 | train acc: 0.8815  val acc: 0.9070 | train loss: 0.5787  val loss: 0.4298 | lr: 0.0100  time: 21.6s\n",
      "noua acuratete de validare: 0.9070\n",
      "epoca  26/60 | train acc: 0.8882  val acc: 0.9118 | train loss: 0.5296  val loss: 0.3861 | lr: 0.0100  time: 26.8s\n",
      "noua acuratete de validare: 0.9118\n",
      "epoca  31/60 | train acc: 0.8929  val acc: 0.9137 | train loss: 0.4964  val loss: 0.3573 | lr: 0.0100  time: 31.8s\n",
      "noua acuratete de validare: 0.9137\n",
      "epoca  36/60 | train acc: 0.8965  val acc: 0.9175 | train loss: 0.4736  val loss: 0.3368 | lr: 0.0100  time: 36.9s\n",
      "noua acuratete de validare: 0.9175\n",
      "epoca  41/60 | train acc: 0.8994  val acc: 0.9188 | train loss: 0.4549  val loss: 0.3215 | lr: 0.0100  time: 42.1s\n",
      "noua acuratete de validare: 0.9188\n",
      "epoca  46/60 | train acc: 0.9023  val acc: 0.9202 | train loss: 0.4415  val loss: 0.3093 | lr: 0.0100  time: 47.2s\n",
      "noua acuratete de validare: 0.9202\n",
      "epoca  51/60 | train acc: 0.9049  val acc: 0.9230 | train loss: 0.4306  val loss: 0.2992 | lr: 0.0100  time: 52.3s\n",
      "noua acuratete de validare: 0.9230\n",
      "epoca  56/60 | train acc: 0.9065  val acc: 0.9243 | train loss: 0.4183  val loss: 0.2904 | lr: 0.0100  time: 57.6s\n",
      "noua acuratete de validare: 0.9243\n",
      "epoca  60/60 | train acc: 0.9082  val acc: 0.9253 | train loss: 0.4126  val loss: 0.2844 | lr: 0.0100  time: 62.0s\n",
      "noua acuratete de validare: 0.9253\n",
      "am ales cele mai bune weighturi\n"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "W1, b1, W2, b2 = initialize_weights(784, HIDDEN_SIZE, 10)\n",
    "\n",
    "n_samples = len(X_train)\n",
    "n_batches = (n_samples + BATCH_SIZE - 1) // BATCH_SIZE  # handle last batch\n",
    "\n",
    "# Training\n",
    "print(f\"incepem training cu {n_samples} si folosim {BATCH_SIZE} per epoca\")\n",
    "print(f\"784 (28x28) -> {HIDDEN_SIZE} (marimea hidden layer ului -> 10\")\n",
    "print(f\"folosim pentur regularizare : dropout={DROPOUT_RATE} si L2={L2_LAMBDA}\")\n",
    "\n",
    "start_time = time.time()\n",
    "lr = LEARNING_RATE\n",
    "best_val_acc = 0\n",
    "k_ast = 0 # de cate ori schimbam learning rate ul \n",
    "best_weights = None\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # dam  un mic shuffle\n",
    "    idx = np.random.permutation(n_samples)\n",
    "    X_shuffled = X_train[idx]\n",
    "    y_shuffled = y_train[idx]\n",
    "    \n",
    "    epoch_loss = 0\n",
    "\n",
    "    #training looo00p\n",
    "    for i in range(n_batches):\n",
    "        inceput = i * BATCH_SIZE\n",
    "        sfarsit = min(inceput + BATCH_SIZE, n_samples)\n",
    "        \n",
    "        X_batch = X_shuffled[inceput:sfarsit]\n",
    "        y_batch = y_shuffled[inceput:sfarsit]\n",
    "        \n",
    "        #faza de ghicire\n",
    "        z1, a1, z2, a2, dropout_mask = forward(\n",
    "            X_batch, W1, b1, W2, b2, \n",
    "            dropout_rate=DROPOUT_RATE, training=True\n",
    "        )\n",
    "        \n",
    "        # calculam lossul pe training\n",
    "        batch_loss = cross_entropy_loss(a2, y_batch, W1, W2, L2_LAMBDA)\n",
    "        epoch_loss += batch_loss * len(X_batch)\n",
    "        \n",
    "        # faza de invatare\n",
    "        dW1, db1, dW2, db2 = backward(\n",
    "            X_batch, y_batch, z1, a1, z2, a2, W1, W2, dropout_mask, L2_LAMBDA\n",
    "        )\n",
    "        \n",
    "        # calculam noile weights \n",
    "        W1 -= lr * dW1\n",
    "        b1 -= lr * db1\n",
    "        W2 -= lr * dW2\n",
    "        b2 -= lr * db2\n",
    "    \n",
    "    # avg loss pt fiecare epoca\n",
    "    epoch_loss /= n_samples\n",
    "    \n",
    "    # raportam ce avem odata la 5 epoci\n",
    "    if epoch % 5 == 0 or epoch == EPOCHS - 1:\n",
    "        train_acc = compute_accuracy(X_train, y_train, W1, b1, W2, b2)\n",
    "        val_acc = compute_accuracy(X_val, y_val, W1, b1, W2, b2)\n",
    "        \n",
    "        \n",
    "        _, _, _, a2_val, _ = forward(X_val, W1, b1, W2, b2, dropout_rate=0, training=False)\n",
    "        val_loss = cross_entropy_loss(a2_val, y_val, W1, W2, L2_LAMBDA)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"epoca {epoch+1:3d}/{EPOCHS} | \"\n",
    "              f\"train acc: {train_acc:.4f}  val acc: {val_acc:.4f} | \"\n",
    "              f\"train loss: {epoch_loss:.4f}  val loss: {val_loss:.4f} | \"\n",
    "              f\"lr: {lr:.4f}  \"\n",
    "              f\"time: {elapsed:.1f}s\")\n",
    "        \n",
    "        #verificam daca, cu lr-ul actual modelul s-a imbunatatit sau nu\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_weights = (W1.copy(), b1.copy(), W2.copy(), b2.copy())\n",
    "            print(f\"noua acuratete de validare: {best_val_acc:.4f}\")\n",
    "            k_ast = 0 # daca s-a imbunatatit, resetam contorul\n",
    "        else:\n",
    "            k_ast += 1 #altfel crestem contorul \n",
    "            if k_ast >= 3: #si aplicam lr decay\n",
    "                lr *= 0.7 #reducem cu 30%\n",
    "                k_ast = 0 #resetam contorul pt urmatorul lr \n",
    "                print(f\"reducem lr la  {lr:.6f}\")\n",
    "\n",
    "\n",
    "#alegem cele mai bune weighturi pt modelul nostru nou \n",
    "if best_weights is not None:\n",
    "    W1, b1, W2, b2 = best_weights\n",
    "    print(\"am ales cele mai bune weighturi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rez\n",
      "acuratetea training: 90.99%\n",
      "acuratetea de validare modelului: 92.53%\n",
      "training loss: 0.3379\n",
      "validation loss loss: 0.2844\n",
      "timp: 62.00s (1.03min)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "train_acc_final = compute_accuracy(X_train_full, y_train_full, W1, b1, W2, b2)\n",
    "val_acc_final = compute_accuracy(X_val, y_val, W1, b1, W2, b2)\n",
    "\n",
    "_, _, _, a2_test, _ = forward(X_test, W1, b1, W2, b2, dropout_rate=0, training=False)\n",
    "predictions = np.argmax(a2_test, axis=1)\n",
    "\n",
    "_, _, _, a2_train, _ = forward(X_train_full, W1, b1, W2, b2, dropout_rate=0, training=False)\n",
    "_, _, _, a2_val, _ = forward(X_val, W1, b1, W2, b2, dropout_rate=0, training=False)\n",
    "train_loss = cross_entropy_loss(a2_train, y_train_full, W1, W2, L2_LAMBDA)\n",
    "val_loss = cross_entropy_loss(a2_val, y_val, W1, W2, L2_LAMBDA)\n",
    "\n",
    "print(f\"rez\")\n",
    "print(f\"acuratetea training: {train_acc_final*100:.2f}%\")\n",
    "print(f\"acuratetea de validare modelului: {val_acc_final*100:.2f}%\")\n",
    "print(f\"training loss: {train_loss:.4f}\")\n",
    "print(f\"validation loss loss: {val_loss:.4f}\")\n",
    "print(f\"timp: {training_time:.2f}s ({training_time/60:.2f}min)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission done\n"
     ]
    }
   ],
   "source": [
    "# Create submission\n",
    "submission = pd.DataFrame({\n",
    "    'ID': range(len(predictions)),\n",
    "    'target': predictions.astype(int)\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"submission done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
