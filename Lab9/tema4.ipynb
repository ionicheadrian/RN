{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ExtendedMNISTDataset(Dataset):\n",
    "    def __init__(self, root: str = \"../input\", train: bool = True):\n",
    "        file_name = \"extended_mnist_test.pkl\" if not train else \"extended_mnist_train.pkl\"\n",
    "        file_path = os.path.join(root, file_name)\n",
    "        \n",
    "        with open(file_path, \"rb\") as fp:\n",
    "            self.data = pickle.load(fp)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i: int):\n",
    "        return self.data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Train: (60000, 784), Test: (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "train_data = []\n",
    "train_labels = []\n",
    "for image, label in ExtendedMNISTDataset(train=True):\n",
    "    train_data.append(image.flatten())\n",
    "    train_labels.append(label)\n",
    "\n",
    "test_data = []\n",
    "for image, label in ExtendedMNISTDataset(train=False):\n",
    "    test_data.append(image.flatten())\n",
    "\n",
    "train_data = np.array(train_data, dtype=np.float32)\n",
    "train_labels = np.array(train_labels, dtype=np.int64)\n",
    "test_data = np.array(test_data, dtype=np.float32)\n",
    "\n",
    "print(f\"Train: {train_data.shape}, Test: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting data...\n",
      "Augmented: (120000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Normalization\n",
    "train_data /= 255.0\n",
    "test_data /= 255.0\n",
    "\n",
    "# Standardization\n",
    "mean = train_data.mean(axis=0, keepdims=True).astype(np.float32)\n",
    "std = train_data.std(axis=0, keepdims=True).astype(np.float32) + 1e-8\n",
    "train_data = (train_data - mean) / std\n",
    "test_data = (test_data - mean) / std\n",
    "\n",
    "# Data Augmentation\n",
    "print(\"Augmenting data...\")\n",
    "augmented_data = [train_data]\n",
    "augmented_labels = [train_labels]\n",
    "\n",
    "# One augmentation with noise\n",
    "noisy = train_data + np.random.normal(0, 0.07, train_data.shape).astype(np.float32)\n",
    "augmented_data.append(noisy)\n",
    "augmented_labels.append(train_labels)\n",
    "\n",
    "train_data_aug = np.vstack(augmented_data).astype(np.float32)\n",
    "train_labels_aug = np.concatenate(augmented_labels).astype(np.int64)\n",
    "\n",
    "print(f\"Augmented: {train_data_aug.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 211, Val batches: 24\n"
     ]
    }
   ],
   "source": [
    "# Train/val split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_data_aug,\n",
    "    train_labels_aug,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify=train_labels_aug\n",
    ")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_t = torch.from_numpy(X_train).float()\n",
    "y_train_t = torch.from_numpy(y_train).long()\n",
    "X_val_t = torch.from_numpy(X_val).float()\n",
    "y_val_t = torch.from_numpy(y_val).long()\n",
    "X_test_t = torch.from_numpy(test_data).float()\n",
    "\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "val_ds = TensorDataset(X_val_t, y_val_t)\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 512\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Total parameters: 569,226\n"
     ]
    }
   ],
   "source": [
    "class FastMLP(nn.Module):\n",
    "    def __init__(self, input_dim=784, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.fc4 = nn.Linear(128, num_classes)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "model = FastMLP(input_dim=train_data.shape[1]).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='max', \n",
    "    factor=0.5, \n",
    "    patience=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            logits = model(xb)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += xb.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training...\n",
      "# 01/25 | training : 92.35% | vall: 97.48%\n",
      "# 02/25 | training : 97.23% | vall: 98.47%\n",
      "# 03/25 | training : 98.09% | vall: 98.88%\n",
      "# 04/25 | training : 98.47% | vall: 99.13%\n",
      "# 05/25 | training : 98.75% | vall: 99.33%\n",
      "# 06/25 | training : 98.99% | vall: 99.40%\n",
      "# 07/25 | training : 99.19% | vall: 99.62%\n",
      "# 08/25 | training : 99.22% | vall: 99.59%\n",
      "# 09/25 | training : 99.33% | vall: 99.59%\n",
      "# 10/25 | training : 99.41% | vall: 99.66%\n",
      "# 11/25 | training : 99.43% | vall: 99.68%\n",
      "# 12/25 | training : 99.47% | vall: 99.72%\n",
      "# 13/25 | training : 99.51% | vall: 99.72%\n",
      "# 14/25 | training : 99.56% | vall: 99.68%\n",
      "# 15/25 | training : 99.59% | vall: 99.77%\n",
      "# 16/25 | training : 99.61% | vall: 99.76%\n",
      "# 17/25 | training : 99.62% | vall: 99.81%\n",
      "# 18/25 | training : 99.66% | vall: 99.79%\n",
      "# 19/25 | training : 99.65% | vall: 99.73%\n",
      "# 20/25 | training : 99.72% | vall: 99.82%\n",
      "# 21/25 | training : 99.67% | vall: 99.82% | LR: 0.001000 , 0.000500\n",
      "# 22/25 | training : 99.83% | vall: 99.89%\n",
      "# 23/25 | training : 99.88% | vall: 99.91%\n",
      "# 24/25 | training : 99.87% | vall: 99.88%\n",
      "# 25/25 | training : 99.88% | vall: 99.88%\n",
      "\n",
      "Training time: 114.75s (1.91 min)\n",
      "Best validation accuracy: 99.91%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining...\")\n",
    "start_training = time.time()\n",
    "\n",
    "epochs = 25\n",
    "best_val_acc = 0.0\n",
    "best_state = None\n",
    "patience = 7\n",
    "no_improve = 0\n",
    "current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_train = 0\n",
    "    correct_train = 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct_train += (preds == yb).sum().item()\n",
    "        total_train += xb.size(0)\n",
    "\n",
    "    train_acc = correct_train / total_train\n",
    "    val_acc = evaluate(val_loader)\n",
    "    \n",
    "    # Check if LR changed\n",
    "    old_lr = current_lr\n",
    "    scheduler.step(val_acc)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    print(f\"# {epoch:02d}/{epochs} | \"\n",
    "          f\"training : {train_acc*100:.2f}% | \"\n",
    "          f\"vall: {val_acc*100:.2f}%\", end=\"\")\n",
    "    \n",
    "    if current_lr != old_lr:\n",
    "        print(f\" | LR: {old_lr:.6f} , {current_lr:.6f}\", end=\"\")\n",
    "    \n",
    "    print()  # New line\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_state = model.state_dict().copy()\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        \n",
    "    if no_improve >= patience:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "training_time = time.time() - start_training\n",
    "print(f\"\\nTraining time: {training_time:.2f}s ({training_time/60:.2f} min)\")\n",
    "print(f\"Best validation accuracy: {best_val_acc*100:.2f}%\")\n",
    "\n",
    "# Load best model\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying TTA...\n",
      "Predictions shape: (10000,)\n",
      "Unique predictions: [0 1 2 3 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nApplying TTA...\")\n",
    "model.eval()\n",
    "tta_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_loader = DataLoader(TensorDataset(X_test_t), batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Original predictions\n",
    "    all_preds = []\n",
    "    for (xb,) in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        logits = model(xb)\n",
    "        all_preds.append(F.softmax(logits, dim=1).cpu().numpy())\n",
    "    tta_predictions.append(np.vstack(all_preds))\n",
    "    \n",
    "    # 2 noisy augmentations\n",
    "    for i in range(2):\n",
    "        X_test_noisy = X_test_t + torch.randn_like(X_test_t) * 0.04\n",
    "        test_loader_noisy = DataLoader(TensorDataset(X_test_noisy), batch_size=batch_size, shuffle=False)\n",
    "        all_preds = []\n",
    "        for (xb,) in test_loader_noisy:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            all_preds.append(F.softmax(logits, dim=1).cpu().numpy())\n",
    "        tta_predictions.append(np.vstack(all_preds))\n",
    "\n",
    "# Average predictions\n",
    "avg_proba = np.mean(tta_predictions, axis=0)\n",
    "final_predictions = np.argmax(avg_proba, axis=1)\n",
    "\n",
    "print(f\"Predictions shape: {final_predictions.shape}\")\n",
    "print(f\"Unique predictions: {np.unique(final_predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timp total: 117.22s (1.95 minutes)\n",
      "\n",
      "Submission saved!\n",
      "   ID  target\n",
      "0   0       7\n",
      "1   1       2\n",
      "2   2       9\n",
      "3   3       5\n",
      "4   4       6\n",
      "5   5       0\n",
      "6   6       5\n",
      "7   7       0\n",
      "8   8       3\n",
      "9   9       0\n"
     ]
    }
   ],
   "source": [
    "# Save submission\n",
    "predictions_csv = {\n",
    "    \"ID\": list(range(len(final_predictions))),\n",
    "    \"target\": final_predictions.tolist(),\n",
    "}\n",
    "df = pd.DataFrame(predictions_csv)\n",
    "df.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"timp total: {total_time:.2f}s ({total_time/60:.2f} minutes)\")\n",
    "print(\"\\nSubmission saved!\")\n",
    "print(df.head(10))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14520005,
     "sourceId": 121334,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
