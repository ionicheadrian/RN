{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ExtendedMNISTDataset(Dataset):\n",
    "    def __init__(self, root: str = \"../input\", train: bool = True):\n",
    "        file_name = \"extended_mnist_test.pkl\" if not train else \"extended_mnist_train.pkl\"\n",
    "        file_path = os.path.join(root, file_name)\n",
    "        \n",
    "        with open(file_path, \"rb\") as fp:\n",
    "            self.data = pickle.load(fp)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i: int):\n",
    "        return self.data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training data\n",
      "loading test data\n"
     ]
    }
   ],
   "source": [
    "print(\"loading training data\")\n",
    "train_data = []\n",
    "train_labels = []\n",
    "for image, label in ExtendedMNISTDataset(train=True):\n",
    "    img = np.array(image, dtype=np.float32).flatten()\n",
    "    train_data.append(img)\n",
    "    train_labels.append(label)\n",
    "\n",
    "print(\"loading test data\")\n",
    "test_data = []\n",
    "for image, label in ExtendedMNISTDataset(train=False):\n",
    "    img = np.array(image, dtype=np.float32).flatten()\n",
    "    test_data.append(img)\n",
    "\n",
    "train_data = np.array(train_data, dtype=np.float32)\n",
    "train_labels = np.array(train_labels, dtype=np.int64)\n",
    "test_data = np.array(test_data, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting data...\n",
      "Augmented train: (180000, 784)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data = train_data / 255.0\n",
    "test_data = test_data / 255.0\n",
    "\n",
    "mean = train_data.mean(axis=0, keepdims=True).astype(np.float32)\n",
    "std = train_data.std(axis=0, keepdims=True).astype(np.float32) + 1e-8\n",
    "train_data = (train_data - mean) / std\n",
    "test_data = (test_data - mean) / std\n",
    "\n",
    "# Data Augmentation\n",
    "print(\"Augmenting data...\")\n",
    "augmented_data = [train_data]\n",
    "augmented_labels = [train_labels]\n",
    "\n",
    "for noise_level in [0.05, 0.1]:\n",
    "    noisy = train_data + np.random.normal(0, noise_level, train_data.shape).astype(np.float32)\n",
    "    augmented_data.append(noisy)\n",
    "    augmented_labels.append(train_labels)\n",
    "\n",
    "train_data_aug = np.vstack(augmented_data).astype(np.float32)\n",
    "train_labels_aug = np.concatenate(augmented_labels).astype(np.int64)\n",
    "\n",
    "print(f\"Augmented train: {train_data_aug.shape}\")\n",
    "\n",
    "# Train/val split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_data_aug,\n",
    "    train_labels_aug,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify=train_labels_aug\n",
    ")\n",
    "\n",
    "# Convert to PyTorch tensors - EXPLICIT float32\n",
    "X_train_t = torch.from_numpy(X_train).float()\n",
    "y_train_t = torch.from_numpy(y_train).long()\n",
    "X_val_t = torch.from_numpy(X_val).float()\n",
    "y_val_t = torch.from_numpy(y_val).long()\n",
    "X_test_t = torch.from_numpy(test_data).float()\n",
    "\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "val_ds = TensorDataset(X_val_t, y_val_t)\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=784, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.fc5 = nn.Linear(128, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "model = MLP(input_dim=train_data.shape[1]).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=0.01,\n",
    "    epochs=50,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    pct_start=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training...\n",
      "#01/30 | loss: 0.2597 | acc: 93.22% | val loss: 0.0730 | val acc: 97.77%\n",
      "#02/30 | loss: 0.0882 | acc: 97.37% | val loss: 0.0485 | val acc: 98.57%\n",
      "#03/30 | loss: 0.0715 | acc: 97.88% | val loss: 0.0393 | val acc: 98.82%\n",
      "#04/30 | loss: 0.0636 | acc: 98.05% | val loss: 0.0402 | val acc: 98.86%\n",
      "#05/30 | loss: 0.0570 | acc: 98.26% | val loss: 0.0358 | val acc: 98.85%\n",
      "#06/30 | loss: 0.0522 | acc: 98.39% | val loss: 0.0324 | val acc: 98.99%\n",
      "#07/30 | loss: 0.0467 | acc: 98.58% | val loss: 0.0291 | val acc: 99.18%\n",
      "#08/30 | loss: 0.0458 | acc: 98.60% | val loss: 0.0246 | val acc: 99.26%\n",
      "#09/30 | loss: 0.0412 | acc: 98.75% | val loss: 0.0251 | val acc: 99.32%\n",
      "#10/30 | loss: 0.0394 | acc: 98.81% | val loss: 0.0226 | val acc: 99.21%\n",
      "#11/30 | loss: 0.0348 | acc: 98.92% | val loss: 0.0256 | val acc: 99.28%\n",
      "#12/30 | loss: 0.0335 | acc: 98.97% | val loss: 0.0159 | val acc: 99.58%\n",
      "#13/30 | loss: 0.0311 | acc: 99.03% | val loss: 0.0178 | val acc: 99.51%\n",
      "#14/30 | loss: 0.0295 | acc: 99.09% | val loss: 0.0107 | val acc: 99.67%\n",
      "#15/30 | loss: 0.0255 | acc: 99.21% | val loss: 0.0134 | val acc: 99.56%\n"
     ]
    }
   ],
   "source": [
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_sum = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss_sum += loss.item() * xb.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += xb.size(0)\n",
    "    return loss_sum / total, correct / total\n",
    "\n",
    "epochs = 20\n",
    "best_val_acc = 0.0\n",
    "best_state = None\n",
    "patience = 10\n",
    "no_improve = 0\n",
    "\n",
    "print(\"\\nTraining...\")\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_train = 0\n",
    "    correct_train = 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct_train += (preds == yb).sum().item()\n",
    "        total_train += xb.size(0)\n",
    "\n",
    "    train_loss = running_loss / total_train\n",
    "    train_acc = correct_train / total_train\n",
    "\n",
    "    val_loss, val_acc = evaluate(val_loader)\n",
    "\n",
    "    print(f\"#{epoch:02d}/{epochs} | \"\n",
    "          f\"loss: {train_loss:.4f} | acc: {train_acc*100:.2f}% | \"\n",
    "          f\"val loss: {val_loss:.4f} | val acc: {val_acc*100:.2f}%\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_state = model.state_dict().copy()\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        \n",
    "    if no_improve >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nBest validation accuracy: {best_val_acc*100:.2f}%\")\n",
    "\n",
    "# Load best model\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "# 7. Test Time Augmentation\n",
    "print(\"TTA\")\n",
    "model.eval()\n",
    "tta_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Original\n",
    "    test_loader = DataLoader(TensorDataset(X_test_t), batch_size=batch_size, shuffle=False)\n",
    "    all_preds = []\n",
    "    for (xb,) in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        logits = model(xb)\n",
    "        all_preds.append(F.softmax(logits, dim=1).cpu().numpy())\n",
    "    tta_predictions.append(np.vstack(all_preds))\n",
    "    \n",
    "    # With noise augmentation\n",
    "    for i in range(4):\n",
    "        X_test_noisy = X_test_t + torch.randn_like(X_test_t) * 0.05\n",
    "        test_loader_noisy = DataLoader(TensorDataset(X_test_noisy), batch_size=batch_size, shuffle=False)\n",
    "        all_preds = []\n",
    "        for (xb,) in test_loader_noisy:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            all_preds.append(F.softmax(logits, dim=1).cpu().numpy())\n",
    "        tta_predictions.append(np.vstack(all_preds))\n",
    "\n",
    "# Average predictions\n",
    "avg_proba = np.mean(tta_predictions, axis=0)\n",
    "final_predictions = np.argmax(avg_proba, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save submission\n",
    "predictions_csv = {\n",
    "    \"ID\": list(range(len(final_predictions))),\n",
    "    \"target\": final_predictions.tolist(),\n",
    "}\n",
    "df = pd.DataFrame(predictions_csv)\n",
    "df.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"submission saved\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14520005,
     "sourceId": 121334,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
